{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":25993,"status":"ok","timestamp":1704853837624,"user":{"displayName":"Nazanin Shafiabadi","userId":"15681583129704549849"},"user_tz":-60},"id":"bljHOyIaeuP0"},"outputs":[],"source":["%pip install datasets"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12418,"status":"ok","timestamp":1704853850029,"user":{"displayName":"Nazanin Shafiabadi","userId":"15681583129704549849"},"user_tz":-60},"id":"6P8tU6_dY9yu","outputId":"6a92900e-4bc2-461f-c542-dc5edb81a71b"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import torch\n","import nltk\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from torch.utils.data import DataLoader\n","from datasets import load_dataset\n","from tqdm.notebook import tqdm\n","\n","# The models the authors used:\n","try:\n","    from transformers import BertConfig, BertForMaskedLM, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n","    from transformers import AlbertForMaskedLM, AlbertTokenizer\n","except ModuleNotFoundError:\n","    %pip install transformers\n","    from transformers import BertForMaskedLM, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n","    from transformers import AlbertForMaskedLM, AlbertTokenizer\n","\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":302,"status":"ok","timestamp":1704853850325,"user":{"displayName":"Nazanin Shafiabadi","userId":"15681583129704549849"},"user_tz":-60},"id":"XAZUT7rXMBeO","outputId":"984b41e0-174a-4c53-9965-8d41eb7339be"},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","DEVICE"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":120,"referenced_widgets":["0261aed79e7e41e48096af087799871e","ac1ee9fb782445d69f0c8abca05f4580","6083f56fd9b24c2fa0c68f8933bba7d9","6c0c6b589a164b5d8446507bebf5ee4d","990e6a9f1e2246a7b1cb89cd5c8678ec","d24f9a002eb5470cbca9d0e42e126a66","44836d782f5b44f082901b3b2084f562","ac616e80219c4a95b1ccec99c3a28e5a","e502507fea8845aa8680fdb6808d4ef8","1c3623b18e59452d92ede28284059204","ff1e5408a9704973ad7c008280bb6627"]},"executionInfo":{"elapsed":543,"status":"ok","timestamp":1704853871276,"user":{"displayName":"Nazanin Shafiabadi","userId":"15681583129704549849"},"user_tz":-60},"id":"9tmYbuTDg7Yh","outputId":"dfaba363-4f27-4147-c958-2b56f18699f8"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0261aed79e7e41e48096af087799871e","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["Dataset({\n","    features: ['annotators_ids', 'scores', 'summary', 'text'],\n","    num_rows: 300\n","})"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["DailyNews_ds = load_dataset('json', data_files='../datasets/DailyNews_300.json', split='train')\n","DailyNews_ds"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3888,"status":"ok","timestamp":1704853913582,"user":{"displayName":"Nazanin Shafiabadi","userId":"15681583129704549849"},"user_tz":-60},"id":"01z4jmL7Y9yy","outputId":"d19897fc-c584-42cb-c5fb-366910fa1a00"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n","model = BertForMaskedLM.from_pretrained('bert-base-uncased').to(DEVICE)"]},{"cell_type":"code","execution_count":50,"metadata":{"executionInfo":{"elapsed":184,"status":"ok","timestamp":1704854873854,"user":{"displayName":"Nazanin Shafiabadi","userId":"15681583129704549849"},"user_tz":-60},"id":"TcyEsBIwBN8l"},"outputs":[],"source":["class DataCollator:\n","    def __init__(self, tokenizer):\n","        self.tokenizer = tokenizer\n","\n","    def __call__(self, batch):\n","\n","        summaries, texts = zip(*[(item['summary'], item['text']) for item in batch])\n","\n","        # Pad summaries\n","\n","        summaries_ids = self.tokenizer(\n","            summaries,\n","            add_special_tokens=False,\n","            truncation=True,\n","            padding='longest',\n","            return_tensors='pt')['input_ids']\n","\n","\n","        # Pad texts\n","\n","        # Tokenizing each text into a list of sentences\n","        texts = [sent_tokenize(text.strip()) for text in texts] # List[List[str]]\n","\n","        # Finding the maximum text length across all texts in the batch and the maximum sentence length across all texts\n","        max_text_len = max(len(text) for text in texts)\n","        max_sent_len = max(max(len(tokenizer.tokenize(sent)) for sent in text) for text in texts)\n","\n","        # Padding each text with empty sentences to make them equal in length\n","        padded_texts = [text + [''] * (max_text_len - len(text)) for text in texts]\n","\n","        # Tokenizing each sentence independently within each text\n","        tokenized_texts = []\n","        for text in padded_texts:\n","            text_tokens = self.tokenizer(text,\n","                                         add_special_tokens=False,\n","                                         truncation=True,\n","                                         padding='max_length',\n","                                         max_length=max_sent_len,\n","                                         return_tensors='pt')\n","\n","            tokenized_texts.append(text_tokens['input_ids'])\n","\n","        # Stacking the padded and tokenized texts along the first dimension to get a tensor of shape [batch_size, num_sentences, max_sentence_length]\n","        texts_ids = torch.stack(tokenized_texts, dim=0)\n","\n","        return {'summaries_ids': summaries_ids, 'texts_ids': texts_ids}"]},{"cell_type":"code","execution_count":56,"metadata":{"executionInfo":{"elapsed":213,"status":"ok","timestamp":1704855231986,"user":{"displayName":"Nazanin Shafiabadi","userId":"15681583129704549849"},"user_tz":-60},"id":"fldWyk25IcEO"},"outputs":[],"source":["def BLANC_help(model, dataloader, M=6, L_min=4, device='cpu'):\n","    \"\"\"\n","    Calculates BLANC similarity between summaries and texts using a BERT-type model.\n","\n","    Parameters:\n","    - model: BERT-type model.\n","    - dataloader: DataLoader instance containing batches of data with 'summaries_ids' and 'texts_ids'.\n","    - M (int, optional): Parameter M for the algorithm (default is 6).\n","    - L_min (int, optional): Minimum length requirement for masked words (default is 4).\n","    - device (str, optional): Device on which to perform computations ('cpu' or 'cuda'). Default is 'cpu'.\n","\n","    Returns:\n","    - List[float]: A list of BLANC similarity scores for each text in the dataset.\n","    \"\"\"\n","\n","    scores = []\n","\n","    for batch in dataloader:\n","        summaries = batch['summaries_ids'].to(device) # Shape: [batch_size, max_summary_length]\n","        texts = batch['texts_ids'].to(device) # Shape: [batch_size, num_sentences, max_sentence_length]\n","\n","        batch_size, num_sentences, max_sentence_length = texts.size()\n","        max_summary_length = summaries.size(1)\n","\n","        filler = torch.zeros_like(summaries).fill_(tokenizer.convert_tokens_to_ids('.'))  # Shape: [batch_size, max_summary_length]\n","\n","        # Initializing S for each text in the batch\n","        S = torch.zeros((batch_size, 2, 2), dtype=torch.float)\n","\n","        for i in range(M):\n","            mask_indices = torch.arange(max_sentence_length).expand(batch_size, num_sentences, -1).to(device)\n","            masked_texts = texts.clone()\n","            mask = (mask_indices % M == 0) & (masked_texts != tokenizer.pad_token_id) # TODO: add a condition to check for the length of the tokens\n","            masked_texts[mask] = tokenizer.mask_token_id  # Shape: [batch_size, num_sentences, max_sentence_length]\n","\n","            # Expanding filler and summaries along the second dimension to match num_sentences\n","            expanded_filler = filler.unsqueeze(1).expand(-1, num_sentences, -1)       # Shape: [batch_size, num_sentences, max_summary_length]\n","            expanded_summaries = summaries.unsqueeze(1).expand(-1, num_sentences, -1) # Shape: [batch_size, num_sentences, max_summary_length]\n","\n","            input_base = torch.cat((expanded_filler, masked_texts), dim=2).to(device)    # Shape: [batch_size, num_sentences, max_summary_length + max_sentence_length]\n","            input_help = torch.cat((expanded_summaries, masked_texts), dim=2).to(device) # Shape: [batch_size, num_sentences, max_summary_length + max_sentence_length]\n","\n","            # The model expects input shapes to be [batch_size, seq_length]\n","            out_base_ids_list = []\n","            out_help_ids_list = []\n","            for sent_idx in range(num_sentences):\n","                sent_input_base = input_base[:, sent_idx, :] # Shape: [batch_size, max_summary_length + max_sentence_length]\n","                sent_input_help = input_help[:, sent_idx, :] # Shape: [batch_size, max_summary_length + max_sentence_length]\n","\n","                attention_mask_base = (sent_input_base != tokenizer.pad_token_id)\n","                attention_mask_help = (sent_input_help != tokenizer.pad_token_id)\n","\n","                with torch.no_grad():\n","                  out_base_logits = model(input_ids=sent_input_base, attention_mask=attention_mask_base).logits  # Shape: [batch_size, max_summary_length + max_sentence_length, Bert_vocab_size]\n","                  out_help_logits = model(input_ids=sent_input_help, attention_mask=attention_mask_help).logits  # Shape: [batch_size, max_summary_length + max_sentence_length, Bert_vocab_size]\n","\n","                # Getting predicted token IDs\n","                out_base_ids_list.append(out_base_logits.argmax(dim=-1))\n","                out_help_ids_list.append(out_help_logits.argmax(dim=-1))\n","\n","            out_base = torch.stack(out_base_ids_list, dim=1) # Shape: [batch_size, num_sentences, max_summary_length + max_sentence_length]\n","            out_help = torch.stack(out_help_ids_list, dim=1) # Shape: [batch_size, num_sentences, max_summary_length + max_sentence_length]\n","\n","            # Indices of masked tokens\n","            masked_indices = mask.nonzero()\n","\n","            for idx in masked_indices:\n","                batch_idx, sentence_idx, token_idx = idx\n","\n","                out_base_token = out_base[batch_idx, sentence_idx, max_summary_length + token_idx].item()\n","                out_help_token = out_help[batch_idx, sentence_idx, max_summary_length + token_idx].item()\n","                text_token = texts[batch_idx, sentence_idx, token_idx].item()\n","\n","                # print(f'out_base_token: {tokenizer.convert_ids_to_tokens(out_base_token)}')\n","                # print(f'out_help_token: {tokenizer.convert_ids_to_tokens(out_help_token)}')\n","                # print(f'text_token: {tokenizer.convert_ids_to_tokens(text_token)}')\n","\n","                # Calculate k and m\n","                k = int(out_base_token == text_token)\n","                m = int(out_help_token == text_token)\n","                S[batch_idx, k, m] += 1\n","                # print(f'S[{batch_idx}, {k}, {m}]: {S[batch_idx, k, m]}')\n","\n","        # Computing scores for each text in the batch, but setting 0.0 for batches with zero denominators to avoid ZeroDivisionError\n","        denominator = S[:, 0, 0] + S[:, 1, 1] + S[:, 0, 1] + S[:, 1, 0]\n","        nonzero_mask = denominator != 0.0\n","        B = torch.zeros_like(denominator, dtype=torch.float)\n","        B[nonzero_mask] = (S[:, 0, 1] - S[:, 1, 0])[nonzero_mask] / denominator[nonzero_mask]\n","\n","        print(f'B: {B.tolist()}')\n","        # Extending the scores list with the scores for the texts in the current batch\n","        scores.extend(B.tolist())\n","\n","    return scores\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4787,"status":"ok","timestamp":1704855238731,"user":{"displayName":"Nazanin Shafiabadi","userId":"15681583129704549849"},"user_tz":-60},"id":"0Y4xznq9LqbV","outputId":"a399f957-c2c5-4abe-bebc-20ec56d9c50a"},"outputs":[],"source":["dataset = DailyNews_ds.select_columns(['summary', 'text'])\n","data_collator = DataCollator(tokenizer)\n","\n","batch_size = 32\n","\n","dataloader = DataLoader(\n","    dataset, batch_size=batch_size, collate_fn=data_collator, shuffle=True\n","    )\n","\n","scores = BLANC_help(model, dataloader, M=6, L_min=4, device=DEVICE)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0261aed79e7e41e48096af087799871e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ac1ee9fb782445d69f0c8abca05f4580","IPY_MODEL_6083f56fd9b24c2fa0c68f8933bba7d9","IPY_MODEL_6c0c6b589a164b5d8446507bebf5ee4d"],"layout":"IPY_MODEL_990e6a9f1e2246a7b1cb89cd5c8678ec"}},"1c3623b18e59452d92ede28284059204":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44836d782f5b44f082901b3b2084f562":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6083f56fd9b24c2fa0c68f8933bba7d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac616e80219c4a95b1ccec99c3a28e5a","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e502507fea8845aa8680fdb6808d4ef8","value":1}},"6c0c6b589a164b5d8446507bebf5ee4d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c3623b18e59452d92ede28284059204","placeholder":"​","style":"IPY_MODEL_ff1e5408a9704973ad7c008280bb6627","value":" 300/0 [00:00&lt;00:00, 3278.05 examples/s]"}},"990e6a9f1e2246a7b1cb89cd5c8678ec":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac1ee9fb782445d69f0c8abca05f4580":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d24f9a002eb5470cbca9d0e42e126a66","placeholder":"​","style":"IPY_MODEL_44836d782f5b44f082901b3b2084f562","value":"Generating train split: "}},"ac616e80219c4a95b1ccec99c3a28e5a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"d24f9a002eb5470cbca9d0e42e126a66":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e502507fea8845aa8680fdb6808d4ef8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ff1e5408a9704973ad7c008280bb6627":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
