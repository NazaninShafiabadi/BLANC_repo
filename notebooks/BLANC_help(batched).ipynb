{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":25993,"status":"ok","timestamp":1704853837624,"user":{"displayName":"Nazanin Shafiabadi","userId":"15681583129704549849"},"user_tz":-60},"id":"bljHOyIaeuP0"},"outputs":[],"source":["%pip install datasets"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12418,"status":"ok","timestamp":1704853850029,"user":{"displayName":"Nazanin Shafiabadi","userId":"15681583129704549849"},"user_tz":-60},"id":"6P8tU6_dY9yu","outputId":"6a92900e-4bc2-461f-c542-dc5edb81a71b"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import torch\n","import nltk\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from torch.utils.data import DataLoader\n","from datasets import load_dataset\n","from tqdm.notebook import tqdm\n","\n","# The models the authors used:\n","try:\n","    from transformers import BertForMaskedLM, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n","    from transformers import AlbertForMaskedLM, AlbertTokenizer\n","except ModuleNotFoundError:\n","    %pip install transformers\n","    from transformers import BertForMaskedLM, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n","    from transformers import AlbertForMaskedLM, AlbertTokenizer\n","\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":302,"status":"ok","timestamp":1704853850325,"user":{"displayName":"Nazanin Shafiabadi","userId":"15681583129704549849"},"user_tz":-60},"id":"XAZUT7rXMBeO","outputId":"984b41e0-174a-4c53-9965-8d41eb7339be"},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","DEVICE"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":120,"referenced_widgets":["0261aed79e7e41e48096af087799871e","ac1ee9fb782445d69f0c8abca05f4580","6083f56fd9b24c2fa0c68f8933bba7d9","6c0c6b589a164b5d8446507bebf5ee4d","990e6a9f1e2246a7b1cb89cd5c8678ec","d24f9a002eb5470cbca9d0e42e126a66","44836d782f5b44f082901b3b2084f562","ac616e80219c4a95b1ccec99c3a28e5a","e502507fea8845aa8680fdb6808d4ef8","1c3623b18e59452d92ede28284059204","ff1e5408a9704973ad7c008280bb6627"]},"executionInfo":{"elapsed":543,"status":"ok","timestamp":1704853871276,"user":{"displayName":"Nazanin Shafiabadi","userId":"15681583129704549849"},"user_tz":-60},"id":"9tmYbuTDg7Yh","outputId":"dfaba363-4f27-4147-c958-2b56f18699f8"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0261aed79e7e41e48096af087799871e","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["Dataset({\n","    features: ['annotators_ids', 'scores', 'summary', 'text'],\n","    num_rows: 300\n","})"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["DailyNews_ds = load_dataset('json', data_files='../datasets/DailyNews_300.json', split='train')\n","DailyNews_ds"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3888,"status":"ok","timestamp":1704853913582,"user":{"displayName":"Nazanin Shafiabadi","userId":"15681583129704549849"},"user_tz":-60},"id":"01z4jmL7Y9yy","outputId":"d19897fc-c584-42cb-c5fb-366910fa1a00"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n","model = BertForMaskedLM.from_pretrained('bert-base-uncased').to(DEVICE)"]},{"cell_type":"code","execution_count":50,"metadata":{"executionInfo":{"elapsed":184,"status":"ok","timestamp":1704854873854,"user":{"displayName":"Nazanin Shafiabadi","userId":"15681583129704549849"},"user_tz":-60},"id":"TcyEsBIwBN8l"},"outputs":[],"source":["class DataCollator:\n","    def __init__(self, tokenizer):\n","        self.tokenizer = tokenizer\n","\n","    def __call__(self, batch):\n","\n","        summaries, texts = zip(*batch)\n","\n","        max_summary_length = min(max(len(summary) for summary in summaries), 512)\n","\n","        summaries_ids = torch.tensor([self.tokenizer(\n","            summary,\n","            add_special_tokens=False,\n","            truncation=True,\n","            max_length=max_summary_length,\n","            padding='max_length')['input_ids'] for summary in summaries])\n","\n","        max_num_sentences = max(len(sent_tokenize(text.strip())) for text in texts)\n","\n","        texts_ids = []\n","        for text in texts:\n","            tokenized_sentences = [\n","                self.tokenizer(sentence,\n","                               add_special_tokens=False,\n","                               truncation=True,\n","                               max_length=max_summary_length, # for consistency, since I can't get the maximum sentence length easily\n","                               padding='max_length')['input_ids']\n","                for sentence in sent_tokenize(text.strip())\n","            ]\n","            padded_sentences = tokenized_sentences + [[self.tokenizer.pad_token_id] * max_summary_length] * (max_num_sentences - len(tokenized_sentences))  # side-effect: there will be many sentences filled with only pad token IDs\n","            texts_ids.append(padded_sentences)\n","\n","        texts_ids = torch.tensor(texts_ids)\n","\n","        return {'summaries_ids': summaries_ids, 'texts_ids': texts_ids}"]},{"cell_type":"code","execution_count":56,"metadata":{"executionInfo":{"elapsed":213,"status":"ok","timestamp":1704855231986,"user":{"displayName":"Nazanin Shafiabadi","userId":"15681583129704549849"},"user_tz":-60},"id":"fldWyk25IcEO"},"outputs":[],"source":["def BLANC_help(model, dataloader, M=6, L_min=4, device='cpu'):\n","    \"\"\"\n","    Calculate BLANC similarity between summaries and texts using a BERT-type model.\n","\n","    Parameters:\n","    - summaries (Tensor): Tensor of tokenized summaries.\n","    - texts (Tensor): Tensor of tokenized sentences.\n","    - model: BERT-type model.\n","    - M (int): Parameter M for the algorithm (default is 6).\n","    - L_min (int): Minimum length requirement for masked words (default is 4).\n","\n","    Returns:\n","    - Tensor: BLANC similarity scores.\n","    \"\"\"\n","\n","    batch_scores = []\n","\n","    for batch in dataloader:\n","        summaries = batch['summaries_ids'].to(device) # Shape: [batch_size, max_summary_length]\n","        texts = batch['texts_ids'].to(device) # Shape: [batch_size, num_sentences, max_summary_length]\n","        texts = texts.squeeze(1)\n","\n","        # print(f'summaries shape: {summaries.shape}')\n","        # print(f'texts shape: {texts.shape}')\n","\n","        filler = torch.zeros_like(summaries).fill_(tokenizer.convert_tokens_to_ids('.'))  # Shape: [batch_size, max_summary_length]\n","        # print(f'filler shape: {filler.shape}')\n","        S = torch.zeros((2, 2), dtype=torch.float)\n","\n","        for i in range(M):\n","            # masked_texts = torch.where((torch.arange(texts.size(1)) - i) % M == 0 & (texts >= L_min), tokenizer.mask_token_id, texts)\n","            masked_texts = torch.where((torch.arange(texts.size(1)).to(device) - i) % M == 0, tokenizer.mask_token_id, texts).to(device) # Shape: [batch_size, num_sentences, max_summary_length] -- need to find a way to get the word lengths\n","            # print(f'masked_texts shape: {masked_texts.shape}')\n","            input_base = torch.cat((filler, masked_texts), dim=1).to(device)  # Shape: [batch_size, max_summary_length * 2]\n","            input_help = torch.cat((summaries, masked_texts), dim=1).to(device) # Shape: [batch_size, max_summary_length * 2]\n","\n","            # print(f'input_base shape: {input_base.shape}')\n","            # print(f'input_help shape: {input_help.shape}')\n","\n","            tokenized_input_base = model(**{'input_ids': input_base}).logits\n","            tokenized_input_help = model(**{'input_ids': input_help}).logits\n","\n","            masked_tokens = torch.nonzero(masked_texts == tokenizer.mask_token_id, as_tuple=False)  # Shape: sometimes [batch_size=32, 2], sometimes [batch_size=64, 2]\n","            # print(f'masked_tokens shape: {masked_tokens.shape}')\n","\n","            for j in range(masked_tokens.size(0)):\n","                idx_batch, idx_token = masked_tokens[j]\n","                predicted_idx_base = torch.argmax(tokenized_input_base[idx_batch, idx_token]).item()\n","                predicted_idx_help = torch.argmax(tokenized_input_help[idx_batch, idx_token]).item()\n","\n","                predicted_word_base = tokenizer.convert_ids_to_tokens(predicted_idx_base)\n","                predicted_word_help = tokenizer.convert_ids_to_tokens(predicted_idx_help)\n","\n","                k = int(predicted_word_base == texts[idx_batch, idx_token])\n","                m = int(predicted_word_help == texts[idx_batch, idx_token])\n","                S[k, m] += 1\n","\n","        print(f'S: {S}')\n","        try:\n","            B = (S[0, 1] - S[1, 0]) / (S[0, 0] + S[1, 1] + S[0, 1] + S[1, 0])\n","        except ZeroDivisionError:\n","            B = torch.zeros(1, dtype=torch.float)\n","        batch_scores.append(B)\n","\n","    avg_B = sum(batch_scores) / len(batch_scores)\n","\n","    return avg_B\n"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4787,"status":"ok","timestamp":1704855238731,"user":{"displayName":"Nazanin Shafiabadi","userId":"15681583129704549849"},"user_tz":-60},"id":"0Y4xznq9LqbV","outputId":"a399f957-c2c5-4abe-bebc-20ec56d9c50a"},"outputs":[{"name":"stdout","output_type":"stream","text":["S: tensor([[224.,   0.],\n","        [  0.,   0.]])\n","S: tensor([[224.,   0.],\n","        [  0.,   0.]])\n","S: tensor([[224.,   0.],\n","        [  0.,   0.]])\n","S: tensor([[224.,   0.],\n","        [  0.,   0.]])\n","S: tensor([[224.,   0.],\n","        [  0.,   0.]])\n","S: tensor([[224.,   0.],\n","        [  0.,   0.]])\n","S: tensor([[224.,   0.],\n","        [  0.,   0.]])\n","S: tensor([[224.,   0.],\n","        [  0.,   0.]])\n","S: tensor([[224.,   0.],\n","        [  0.,   0.]])\n","S: tensor([[84.,  0.],\n","        [ 0.,  0.]])\n"]},{"data":{"text/plain":["tensor(0.)"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["dataset = DailyNews_ds.select_columns(['summary', 'text'])\n","data_collator = DataCollator(tokenizer)\n","\n","batch_size = 32\n","\n","dataloader = DataLoader(\n","    dataset, batch_size=batch_size, collate_fn=data_collator, shuffle=True\n","    )\n","\n","BLANC_help(model, dataloader, M=6, L_min=4, device=DEVICE)"]},{"cell_type":"markdown","metadata":{"id":"i94VJUObY9y0"},"source":["Ideas for improvement:\n","1. try other models\n","2. try other datasets\n","3. test on other problems"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0261aed79e7e41e48096af087799871e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ac1ee9fb782445d69f0c8abca05f4580","IPY_MODEL_6083f56fd9b24c2fa0c68f8933bba7d9","IPY_MODEL_6c0c6b589a164b5d8446507bebf5ee4d"],"layout":"IPY_MODEL_990e6a9f1e2246a7b1cb89cd5c8678ec"}},"1c3623b18e59452d92ede28284059204":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44836d782f5b44f082901b3b2084f562":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6083f56fd9b24c2fa0c68f8933bba7d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac616e80219c4a95b1ccec99c3a28e5a","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e502507fea8845aa8680fdb6808d4ef8","value":1}},"6c0c6b589a164b5d8446507bebf5ee4d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c3623b18e59452d92ede28284059204","placeholder":"​","style":"IPY_MODEL_ff1e5408a9704973ad7c008280bb6627","value":" 300/0 [00:00&lt;00:00, 3278.05 examples/s]"}},"990e6a9f1e2246a7b1cb89cd5c8678ec":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac1ee9fb782445d69f0c8abca05f4580":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d24f9a002eb5470cbca9d0e42e126a66","placeholder":"​","style":"IPY_MODEL_44836d782f5b44f082901b3b2084f562","value":"Generating train split: "}},"ac616e80219c4a95b1ccec99c3a28e5a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"d24f9a002eb5470cbca9d0e42e126a66":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e502507fea8845aa8680fdb6808d4ef8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ff1e5408a9704973ad7c008280bb6627":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
