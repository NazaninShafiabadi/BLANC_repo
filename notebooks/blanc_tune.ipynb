{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5d9f8b7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Liora\\anaconda3\\envs\\blanc\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from transformers import BertTokenizer, BertForMaskedLM, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b872e196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Users\\Liora\\anaconda3\\envs\\blanc\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_json('./datasets/DailyNews_300.json')\n",
    "print(data.shape)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "397b468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.features = self.dataset.columns\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.dataset.iloc[idx, 0], self.dataset.iloc[idx, 1])\n",
    "    \n",
    "    def map(self, preprocessing_fn, **kwargs):\n",
    "        return CustomDataset(self.dataset.apply(lambda x: preprocessing_fn(x, **kwargs), axis = 1))\n",
    "    \n",
    "    def select_columns(self, columns):\n",
    "        new_dataset = self.dataset[columns] \n",
    "        return CustomDataset(new_dataset)\n",
    "    \n",
    "dataset = CustomDataset(data)\n",
    "# print(dataset.__getitem__(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c34df281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_lengths(dataset, tokenizer):\n",
    "    word_lengths = {}\n",
    "    all_tokens = []\n",
    "\n",
    "    for sample in dataset:\n",
    "        summary = sample[0]\n",
    "        preprocessed_result = tokenizer(summary, \n",
    "                                        add_special_tokens = False,\n",
    "                                        truncation = True,\n",
    "                                        max_length = 512,\n",
    "                                        padding = False,\n",
    "                                        return_attention_mask = False)\n",
    "        tokens = preprocessed_result[\"input_ids\"]\n",
    "        decoded_tokens = tokenizer.convert_ids_to_tokens(tokens)\n",
    "        for token in tokens:\n",
    "            if token not in all_tokens:\n",
    "                all_tokens.append(token)\n",
    "\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if decoded_tokens[i].startswith('##'):\n",
    "                combined_word = decoded_tokens[i - 1] + decoded_tokens[i][2:]\n",
    "                word_lengths[tokens[i - 1]] = len(combined_word)\n",
    "                word_lengths[tokens[i]] = len(combined_word)\n",
    "            else:\n",
    "                word_lengths[tokens[i]] = len(decoded_tokens[i])\n",
    "            i += 1\n",
    "\n",
    "    assert len(all_tokens) == len(word_lengths), \"Association of tokens with word length : FAILED.\"\n",
    "\n",
    "    return word_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38e993a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Former England defender Gary Neville suggested Gareth Southgate \\' s squad had done more than could have been expected of them at this World Cup.\\nIvan Perisic and Mario Mandzukic goals earned Croatia a date with France after a 2 - 1 extra - time win in Moscow.\\nNeville : \" This team has taken us to a place we never imagined we could get \"\\nEngland \\' s Dejan Lovren and Domagoj Vida were sent off in the second half.\\nDefender Rio Ferdinand says experience was a telling factor in the England squad.\\nEngland face France in the World Cup final in Brazil.', 'Mario Mandzukic pounces to fire the ball past Jordan Pickford and put Croatia into the World Cup final. Photo: Reuters Independent.ie \\n \\nFormer England defender Gary Neville suggested Gareth Southgate\\'s squad had done more than could have been expected of them at this World Cup as they bowed out with a semi-final defeat against Croatia. \\n \\nhttps://www.independent.ie/sport/soccer/world-cup-2018/gary-neville-salutes-englands-overachievers-as-alan-shearer-and-rio-ferdinand-give-their-verdicts-37108667.html \\n \\nhttps://www.independent.ie/incoming/article37108634.ece/7571a/AUTOCROP/h342/52Man1.jpg \\n   Email     \\nFormer England defender Gary Neville suggested Gareth Southgate\\'s squad had done more than could have been expected of them at this World Cup as they bowed out with a semi-final defeat against Croatia. \\n  \\nA jaded England faded after had Kieran Trippier fired England ahead after just five minutes with a superb free-kick, with goals from Ivan Perisic and Mario Mandzukic earned Croatia a date with France after a 2-1 extra-time win in Moscow. \\n \\n\"The first thing to say is this team has taken us to a place we never imagined we could get,\" said Neville. \\n \\n\"I said before the tournament England never overachieve, this team has overachieved getting to the semi-final and they can all be absolutely proud. \\n \\n\"They have taken the nation with them. You see all the fans out in the park to see them. I never thought I\\'d see scenes like that in this World Cup. \\n \\n\"They can be absolutely proud. We did say before the game these lads have to take this opportunity because it may never come again. \\n \\n\"All I hope as I look at them now crying, disappointed, upset is they get another opportunity as good as the one they\\'ve just had.\" \\n \\n\"They\\'re the youngest team in the tournament. I hope they\\'re in that position again, 1-0 up with 20 minutes to go with the opportunity of getting into a World Cup final.\" \\n  Croatia\\'s Dejan Lovren and Domagoj Vida celebrate after the match. Photo: Reuters   \\nFormer England defender Rio Ferdinand believes the youthful nature of England\\'s squad proved to be their undoing in the semi-final. \\n \\n\"I think experience was a telling factor today,\" Ferdinand told the BBC. \"This Croatian team had medals all over the squad but now is not the time to knitpick. This bunch of players have brought the nation together and the fans haven\\'t sung like this for years. \\n \\n\"We wanted this team to come out with an identity and a philosophy of playing, and I think they have done that. Where we were sat in the stadium, people around us were impressed with England\\'s football and their ability to play out from the back. \\n \\n\"Kieran Trippier\\'s delivery of the ball was brilliant and what a tournament he has had. There were question marks over Jordan Pickford but he was the best choice in goal and this team can develop into something very strong. I have been an admirer of John Stones for a long time, and he defended unbelievably today. We have got a very inexperienced young squad but they have grown and will continue to do so.\" \\n \\nEx-England striker Alan Shearer also tried to find the positives amid England\\'s anguish. \\n \\n\"We said before the tournament that if they gave absolutely everything in every game, everyone at home would be happy,\" said Shearer. \\n \\n\"England got off to a very good start and dominated the first 45 minutes but Croatia stepped 10 yards further forward in the second half. \\n \\n\"Luka Modric bossed the midfield and we could not get near him with his deft touches and passes, he was superb and a man of the match performance, a vital cog for them. We lacked experience to get someone on him and stop him. Croatia did not panic when they fell behind. \\n \\n\"Kieran Trippier was brilliant in the tournament and so was Jordan Pickford, especially with his delivery. Harry Kane was a bit quiet in the knockout games and he had a chance that he could not put away \\n \\n\"So many positives to take out of the tournament, we did not expect to get to the quarters and will go home after this without heads held high. Finishing third would be excellent. Well done guys, you have given us plenty to shout about.\" \\n \\nOnline Editors')\n",
      "([2280, 2563, 8291, 5639, 14871, 4081, 20243, 2148, 5867, 1005, 1055, 4686, 2018, 2589, 2062, 2084, 2071, 2031, 2042, 3517, 1997, 2068, 2012, 2023, 2088, 2452, 1012, 7332, 2566, 17417, 2278, 1998, 7986, 2158, 2094, 24015, 2278, 3289, 3687, 8097, 1037, 3058, 2007, 2605, 2044, 1037, 1016, 1011, 1015, 4469, 1011, 2051, 2663, 1999, 4924, 1012, 14871, 1024, 1000, 2023, 2136, 2038, 2579, 2149, 2000, 1037, 2173, 2057, 2196, 8078, 2057, 2071, 2131, 1000, 2563, 1005, 1055, 2139, 8405, 8840, 12229, 2078, 1998, 14383, 23692, 3501, 19830, 2020, 2741, 2125, 1999, 1996, 2117, 2431, 1012, 8291, 5673, 9684, 2758, 3325, 2001, 1037, 4129, 5387, 1999, 1996, 2563, 4686, 1012, 2563, 2227, 2605, 1999, 1996, 2088, 2452, 2345, 1999, 4380, 1012], [7986, 2158, 2094, 24015, 2278, 13433, 17457, 2015, 2000, 2543, 1996, 3608, 2627, 5207, 4060, 3877, 1998, 2404, 8097, 2046, 1996, 2088, 2452, 2345, 1012, 6302, 1024, 26665, 2981, 1012, 29464, 2280, 2563, 8291, 5639, 14871, 4081, 20243, 2148, 5867, 1005, 1055, 4686, 2018, 2589, 2062, 2084, 2071, 2031, 2042, 3517, 1997, 2068, 2012, 2023, 2088, 2452, 2004, 2027, 11489, 2041, 2007, 1037, 4100, 1011, 2345, 4154, 2114, 8097, 1012, 16770, 1024, 1013, 1013, 7479, 1012, 2981, 1012, 29464, 1013, 4368, 1013, 4715, 1013, 2088, 1011, 2452, 1011, 2760, 1013, 5639, 1011, 14871, 1011, 17664, 2015, 1011, 2563, 2015, 1011, 2058, 21046, 22507, 2015, 1011, 2004, 1011, 5070, 1011, 18330, 2121, 1011, 1998, 1011, 5673, 1011, 9684, 1011, 2507, 1011, 2037, 1011, 14392, 2015, 1011, 4261, 10790, 20842, 2575, 2581, 1012, 16129, 16770, 1024, 1013, 1013, 7479, 1012, 2981, 1012, 29464, 1013, 14932, 1013, 3720, 24434, 10790, 20842, 22022, 1012, 14925, 2063, 1013, 4293, 2581, 2487, 2050, 1013, 8285, 26775, 7361, 1013, 1044, 22022, 2475, 1013, 4720, 2386, 2487, 1012, 16545, 2290, 10373, 2280, 2563, 8291, 5639, 14871, 4081, 20243, 2148, 5867, 1005, 1055, 4686, 2018, 2589, 2062, 2084, 2071, 2031, 2042, 3517, 1997, 2068, 2012, 2023, 2088, 2452, 2004, 2027, 11489, 2041, 2007, 1037, 4100, 1011, 2345, 4154, 2114, 8097, 1012, 1037, 12323, 2094, 2563, 8105, 2044, 2018, 11382, 23169, 4440, 14756, 2099, 5045, 2563, 3805, 2044, 2074, 2274, 2781, 2007, 1037, 21688, 2489, 1011, 5926, 1010, 2007, 3289, 2013, 7332, 2566, 17417, 2278, 1998, 7986, 2158, 2094, 24015, 2278, 3687, 8097, 1037, 3058, 2007, 2605, 2044, 1037, 1016, 1011, 1015, 4469, 1011, 2051, 2663, 1999, 4924, 1012, 1000, 1996, 2034, 2518, 2000, 2360, 2003, 2023, 2136, 2038, 2579, 2149, 2000, 1037, 2173, 2057, 2196, 8078, 2057, 2071, 2131, 1010, 1000, 2056, 14871, 1012, 1000, 1045, 2056, 2077, 1996, 2977, 2563, 2196, 2058, 21046, 18697, 1010, 2023, 2136, 2038, 2058, 21046, 18697, 2094, 2893, 2000, 1996, 4100, 1011, 2345, 1998, 2027, 2064, 2035, 2022, 7078, 7098, 1012, 1000, 2027, 2031, 2579, 1996, 3842, 2007, 2068, 1012, 2017, 2156, 2035, 1996, 4599, 2041, 1999, 1996, 2380, 2000, 2156, 2068, 1012, 1045, 2196, 2245, 1045, 1005, 1040, 2156, 5019, 2066, 2008, 1999, 2023, 2088, 2452, 1012, 1000, 2027, 2064, 2022, 7078, 7098, 1012, 2057, 2106, 2360, 2077, 1996, 2208, 2122, 29126, 2031, 2000, 2202, 2023, 4495, 2138, 2009, 2089, 2196, 2272, 2153, 1012, 1000, 2035, 1045, 3246, 2004, 1045, 2298, 2012, 2068, 2085, 6933, 1010, 9364, 1010, 6314, 2003, 2027, 2131, 2178, 4495, 2004, 2204, 2004, 1996, 2028, 2027, 1005, 2310, 2074, 2018, 1012, 1000, 1000, 2027, 1005, 2128, 1996, 6587, 2136, 1999, 1996, 2977, 1012, 1045, 3246, 2027, 1005, 2128, 1999, 2008, 2597, 2153, 1010, 1015, 1011, 1014, 2039, 2007, 2322, 2781, 2000, 2175, 2007, 1996, 4495, 1997, 2893, 2046, 1037, 2088, 2452, 2345, 1012, 1000, 8097, 1005, 1055, 2139, 8405, 8840, 12229, 2078, 1998, 14383, 23692, 3501, 19830, 8439, 2044, 1996, 2674, 1012, 6302, 1024, 26665, 2280, 2563, 8291, 5673, 9684, 7164, 1996, 22446, 3267, 1997, 2563, 1005, 1055, 4686, 4928, 2000, 2022, 2037, 25672, 2075, 1999, 1996, 4100, 1011, 2345])\n"
     ]
    }
   ],
   "source": [
    "def preprocessing_fn(x, tokenizer):\n",
    "    x[\"summary_ids\"] = tokenizer(\n",
    "        x[\"summary\"],\n",
    "        add_special_tokens = False,\n",
    "        truncation = True,\n",
    "        max_length = 512,\n",
    "        padding = False,\n",
    "        return_attention_mask = True,\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    x[\"text_ids\"] = tokenizer(\n",
    "        x[\"text\"],\n",
    "        add_special_tokens = False,\n",
    "        truncation = True,\n",
    "        max_length = 512,\n",
    "        padding = False,\n",
    "        return_attention_mask = True,\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    return x\n",
    "\n",
    "splitted_dataset = dataset.select_columns([\"summary\", \"text\"])\n",
    "# print(splitted_dataset.__getitem__(0))\n",
    "\n",
    "word_lengths = get_word_lengths(splitted_dataset, tokenizer)\n",
    "\n",
    "# Tokenize the dataset\n",
    "splitted_dataset = splitted_dataset.map(\n",
    "    preprocessing_fn, tokenizer = tokenizer\n",
    ")\n",
    "print(splitted_dataset.__getitem__(0))\n",
    "\n",
    "# Remove useless columns\n",
    "splitted_dataset = splitted_dataset.select_columns([\"summary_ids\", \"text_ids\"])\n",
    "print(splitted_dataset.__getitem__(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "871c9b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Get the tokenized sequences for each item in the batch\n",
    "    text_ids_batch = [torch.tensor(item[1], dtype = torch.int) for item in batch]\n",
    "    summary_ids_batch = [torch.tensor(item[0], dtype = torch.int) for item in batch]\n",
    "\n",
    "    # Pad sequences to the maximum length in the batch\n",
    "    padded_text_ids = pad_sequence([torch.cat([item, torch.zeros(512 - len(item))]) for item in text_ids_batch], batch_first = True, padding_value = 0)\n",
    "    padded_summary_ids = pad_sequence([torch.cat([item, torch.zeros(512 - len(item))]) for item in summary_ids_batch], batch_first = True, padding_value = 0)\n",
    "\n",
    "    # padded_text_ids = pad_sequence(text_ids_batch, batch_first = True, padding_value = 0, maxlen = 512)\n",
    "    # padded_summary_ids = pad_sequence(summary_ids_batch, batch_first = True, padding_value = 0, maxlen = 512)\n",
    "\n",
    "    return {\"text_ids\": padded_text_ids, \"summary_ids\": padded_summary_ids}\n",
    "\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(splitted_dataset, batch_size = batch_size, collate_fn = collate_fn)\n",
    "\n",
    "epochs = 3\n",
    "def training(summary, text, model, epochs = 10):\n",
    "    model_copy = copy.deepcopy(model)\n",
    "    model_copy.train()\n",
    "\n",
    "    summary = summary.unsqueeze(0)\n",
    "    text = text.unsqueeze(0)\n",
    "    if summary.size(1) != text.size(1):\n",
    "        raise RuntimeError(\"Sizes along the sequence length dimension must match.\")\n",
    "    \n",
    "    for epochs in range(epochs):\n",
    "        whole_input = torch.cat((summary, text), dim = 0).long()\n",
    "        outputs = model_copy(whole_input, labels = whole_input)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return model_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99b978b0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text_ids': tensor([[ 7986.,  2158.,  2094.,  ...,  4100.,  1011.,  2345.],\n",
      "        [ 2005.,  1996.,  3822.,  ...,  2606., 11917.,  3737.],\n",
      "        [ 6264.,  1517.,  1996.,  ...,  2005.,  1996.,  2110.],\n",
      "        ...,\n",
      "        [ 5655., 16216., 19020.,  ..., 19723., 24454.,  3686.],\n",
      "        [ 1996.,  4501.,  2966.,  ..., 12763.,  3378.,  2007.],\n",
      "        [ 2062.,  2084.,  1002.,  ...,  2095.,  1011.,  2214.]]), 'summary_ids': tensor([[ 2280.,  2563.,  8291.,  ...,     0.,     0.,     0.],\n",
      "        [ 1048.,  1005., 10848.,  ...,     0.,     0.,     0.],\n",
      "        [ 2655., 21293.,  2100.,  ...,     0.,     0.,     0.],\n",
      "        ...,\n",
      "        [ 5655., 16216., 19020.,  ...,     0.,     0.,     0.],\n",
      "        [ 2151.,  6926.,  1010.,  ...,     0.,     0.,     0.],\n",
      "        [ 3533.,  7226.,  2368.,  ...,     0.,     0.,     0.]])}\n",
      "Summary 0 of batch\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "9\n",
      "\n",
      "\n",
      "Elapsed Time: 7.783858776092529 seconds\n",
      "Summary 1 of batch\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "\n",
      "\n",
      "Elapsed Time: 7.009265899658203 seconds\n",
      "Summary 2 of batch\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "\n",
      "\n",
      "Elapsed Time: 6.768125057220459 seconds\n",
      "Summary 3 of batch\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "\n",
      "\n",
      "Elapsed Time: 6.481881380081177 seconds\n",
      "Summary 4 of batch\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "\n",
      "\n",
      "Elapsed Time: 6.665089845657349 seconds\n",
      "Summary 5 of batch\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "\n",
      "\n",
      "Elapsed Time: 6.742142200469971 seconds\n",
      "Summary 6 of batch\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "\n",
      "\n",
      "Elapsed Time: 10.321661472320557 seconds\n",
      "Summary 7 of batch\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "\n",
      "\n",
      "Elapsed Time: 8.98961853981018 seconds\n",
      "Summary 8 of batch\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "\n",
      "\n",
      "Elapsed Time: 9.989394187927246 seconds\n",
      "Summary 9 of batch\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "\n",
      "\n",
      "Elapsed Time: 8.77285122871399 seconds\n",
      "Summary 10 of batch\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "\n",
      "\n",
      "Elapsed Time: 8.421805620193481 seconds\n",
      "Summary 11 of batch\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "\n",
      "\n",
      "Elapsed Time: 9.042613744735718 seconds\n",
      "Summary 12 of batch\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "\n",
      "\n",
      "Elapsed Time: 8.717963695526123 seconds\n",
      "Summary 13 of batch\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "\n",
      "\n",
      "Elapsed Time: 9.768409729003906 seconds\n",
      "Summary 14 of batch\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "\n",
      "\n",
      "Elapsed Time: 8.48296856880188 seconds\n",
      "Summary 15 of batch\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "\n",
      "\n",
      "Elapsed Time: 9.30433201789856 seconds\n",
      "Summary 16 of batch\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "\n",
      "\n",
      "Elapsed Time: 9.288742065429688 seconds\n",
      "Summary 17 of batch\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "\n",
      "\n",
      "Elapsed Time: 8.148532390594482 seconds\n",
      "Summary 18 of batch\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "\n",
      "\n",
      "Elapsed Time: 8.08956789970398 seconds\n",
      "Summary 19 of batch\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "\n",
      "\n",
      "Elapsed Time: 8.180822610855103 seconds\n",
      "Summary 20 of batch\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[1;32m---> 39\u001b[0m     tuned_models \u001b[38;5;241m=\u001b[39m \u001b[43mblanc_tune_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 29\u001b[0m, in \u001b[0;36mblanc_tune_batch\u001b[1;34m(batch, model, p_mask, l_min, N, epochs)\u001b[0m\n\u001b[0;32m     27\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     28\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 29\u001b[0m tuned_model \u001b[38;5;241m=\u001b[39m \u001b[43mblanc_tune\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     31\u001b[0m batch_tuned_models\u001b[38;5;241m.\u001b[39mappend(tuned_model)\n",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m, in \u001b[0;36mblanc_tune\u001b[1;34m(summary, text, model, p_mask, l_min, N, epochs)\u001b[0m\n\u001b[0;32m     14\u001b[0m             set_tune\u001b[38;5;241m.\u001b[39mloc[set_tune\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m [masked_summary, text]\n\u001b[0;32m     15\u001b[0m         pos \u001b[38;5;241m=\u001b[39m pos[N_mask:]\n\u001b[1;32m---> 17\u001b[0m model_tuned \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)      \n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_tuned\n",
      "Cell \u001b[1;32mIn[7], line 32\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(summary, text, model, epochs)\u001b[0m\n\u001b[0;32m     30\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model_copy(whole_input, labels \u001b[38;5;241m=\u001b[39m whole_input)\n\u001b[0;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Liora\\anaconda3\\envs\\blanc\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Liora\\anaconda3\\envs\\blanc\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def blanc_tune(summary, text, model, p_mask = 0.15, l_min = 4, N = 10, epochs = 10):\n",
    "    N_summary = len(summary)\n",
    "    N_mask = int(N_summary*p_mask)\n",
    "    set_tune = pd.DataFrame(columns = ['summary', 'text'])\n",
    "\n",
    "    for j in range(0, N):\n",
    "        print(j)\n",
    "        pos = [i for i, token in enumerate(summary.tolist()) if token in word_lengths and word_lengths[token] >= l_min]\n",
    "        random.shuffle(pos)\n",
    "        while len(pos) != 0:\n",
    "            masked_summary = summary.tolist().copy()\n",
    "            for pos_to_mask in pos[:N_mask]:\n",
    "                masked_summary[pos_to_mask] = '<MASK>'\n",
    "                set_tune.loc[set_tune.shape[0]] = [masked_summary, text]\n",
    "            pos = pos[N_mask:]\n",
    "\n",
    "    model_tuned = training(summary, text, model, epochs)\n",
    "    print('\\n')      \n",
    "    return model_tuned\n",
    "\n",
    "def blanc_tune_batch(batch, model, p_mask = 0.15, l_min = 4, N = 10, epochs = 10):\n",
    "    batch_tuned_models = []\n",
    "    \n",
    "    i = 0\n",
    "    for summary, text in zip(batch['summary_ids'], batch['text_ids']):\n",
    "        print(f\"Summary {i} of batch\")\n",
    "        i += 1\n",
    "        start_time = time.time()\n",
    "        tuned_model = blanc_tune(summary, text, model, p_mask, l_min, N, epochs)\n",
    "        end_time = time.time()\n",
    "        batch_tuned_models.append(tuned_model)\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Elapsed Time: {elapsed_time} seconds\")\n",
    "\n",
    "    return batch_tuned_models\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    tuned_models = blanc_tune_batch(batch, model, epochs = epochs)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "collabfiltering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
