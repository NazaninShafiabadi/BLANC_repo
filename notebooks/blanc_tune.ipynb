{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a5d9f8b7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Liora\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from transformers import BertTokenizer, BertForMaskedLM, AdamW\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import copy\n",
    "import time\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b872e196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(555, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Users\\Liora\\anaconda3\\envs\\blanc\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "filename = \"CNN_DailyMail_555.json\"\n",
    "# filename = \"DailyNews_300.json\"\n",
    "data = pd.read_json('../datasets/' + filename)\n",
    "print(data.shape)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "397b468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.features = self.dataset.columns\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.dataset.iloc[idx, 0], \n",
    "                self.dataset.iloc[idx, 1], \n",
    "                self.dataset.iloc[idx, 2], \n",
    "                self.dataset.iloc[idx, 3])\n",
    "    \n",
    "    def map(self, preprocessing_fn, **kwargs):\n",
    "        return CustomDataset(self.dataset.apply(lambda x: preprocessing_fn(x, **kwargs), axis = 1))\n",
    "    \n",
    "    def select_columns(self, columns):\n",
    "        new_dataset = self.dataset[columns] \n",
    "        return CustomDataset(new_dataset)\n",
    "    \n",
    "    def get_sentences(self):\n",
    "        self.dataset['sentences'] = self.dataset['text'].apply(lambda x: sent_tokenize(x))\n",
    "        return CustomDataset(self.dataset)\n",
    "    \n",
    "    # Data cleaning\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        # lower case\n",
    "        text = text.lower()\n",
    "    \n",
    "        # before normalization : manual handling of contractions and line breaks\n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = text.replace(' \\' ', '\\'')\n",
    "        text = text.replace('\\'', '')\n",
    "    \n",
    "        # string normalization.\n",
    "        text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore')\n",
    "        text = str(text)[2:-1]\n",
    "        # the result of previous line adds a few characters to the string,\n",
    "        # we remove them.\n",
    "    \n",
    "        # remove non alpha numeric characters, except dots, question and exclamation marks that will be needed to separate sentences.\n",
    "        text = re.sub(r'[^\\w]', ' ', text)\n",
    "    \n",
    "        # replace numbers by the <NUM> token.\n",
    "        text = re.sub(r'[0-9]+', '<NUM>', text)\n",
    "    \n",
    "        # remove double whitespaces.\n",
    "        text = re.sub(r'( ){2,}', ' ', text).strip()\n",
    "        # removing spaces at beginning and end of string.\n",
    "    \n",
    "        return text\n",
    "    \n",
    "    def apply_preprocess(self):\n",
    "        self.dataset[\"summary\"] = self.dataset['summary'].apply(lambda x: self.preprocess_text(x))\n",
    "        self.dataset[\"text\"] = self.dataset['text'].apply(lambda x: self.preprocess_text(x))\n",
    "        new_sentences_col = []\n",
    "        for sentences in self.dataset['sentences']:\n",
    "            new_sentences_col.append([self.preprocess_text(sentence) for sentence in sentences])\n",
    "        self.dataset['sentences'] = new_sentences_col\n",
    "        return CustomDataset(self.dataset)\n",
    "    \n",
    "    def random_words_summary(self, summary):\n",
    "        random_summary = \"\"\n",
    "        summary = summary.split()\n",
    "        for _ in range(len(summary)):\n",
    "            random_summary += random.choice(summary) + ' '\n",
    "        return random_summary\n",
    "    \n",
    "    def apply_random_words_summary(self):\n",
    "        self.dataset['random_summary'] = self.dataset['summary'].apply(lambda x: self.random_words_summary(x))\n",
    "        return CustomDataset(self.dataset)\n",
    "    \n",
    "dataset = CustomDataset(data)\n",
    "dataset = dataset.get_sentences()\n",
    "# print(dataset.__getitem__(0)[2])\n",
    "# print(dataset.__getitem__(0)[3])\n",
    "# print(dataset.__getitem__(0)[4])\n",
    "dataset = dataset.apply_preprocess()\n",
    "dataset = dataset.apply_random_words_summary()\n",
    "# print(dataset.__getitem__(0)[2])\n",
    "# print(dataset.__getitem__(0)[3])\n",
    "# print(dataset.__getitem__(0)[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c34df281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_lengths(dataset, tokenizer, l_min = 4):\n",
    "    word_lengths = {}\n",
    "    all_tokens = []\n",
    "\n",
    "    for sample in dataset:\n",
    "        summary = sample[0]\n",
    "        preprocessed_result = tokenizer(summary, \n",
    "                                        add_special_tokens = False,\n",
    "                                        truncation = True,\n",
    "                                        max_length = 512,\n",
    "                                        padding = False,\n",
    "                                        return_attention_mask = False)\n",
    "        tokens = preprocessed_result[\"input_ids\"]\n",
    "        decoded_tokens = tokenizer.convert_ids_to_tokens(tokens)\n",
    "        for token in tokens:\n",
    "            if token not in all_tokens:\n",
    "                all_tokens.append(token)\n",
    "\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            eligible = False\n",
    "            if decoded_tokens[i].startswith('##'):\n",
    "                eligible = True\n",
    "                word_lengths[tokens[i - 1]] = eligible\n",
    "                word_lengths[tokens[i]] = eligible\n",
    "            else:\n",
    "                if len(decoded_tokens[i]) >= l_min:\n",
    "                    eligible = True\n",
    "                word_lengths[tokens[i]] = eligible\n",
    "            i += 1\n",
    "\n",
    "    assert len(all_tokens) == len(word_lengths), \"Association of tokens with word length : FAILED.\"\n",
    "\n",
    "    return word_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "38e993a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2079, 3501, 2089, 2145, 3288, 5571, 2114, 2577, 27946, 2079, 3501, 2071, 3288, 2976, 2942, 5571, 2114, 27946, 2942, 2916, 5160, 3421, 11851, 17789, 19953, 3008]\n",
      "[1996, 2533, 1997, 3425, 2089, 2145, 3288, 2942, 5571, 2114, 11851, 17789, 3235, 13108, 2577, 27946, 4905, 2236, 4388, 9111, 3936, 2006, 9432, 9111, 2409, 12060, 2006, 9432, 2008, 2079, 3501, 2018, 2025, 5531, 2049, 4812, 2046, 1996, 2337, 1026, 16371, 2213, 1028, 1026, 16371, 2213, 1028, 5043, 1998, 1996, 3043, 2003, 7552, 2045, 2024, 3161, 4084, 2008, 2057, 2024, 2145, 1999, 1996, 2832, 1997, 2635, 9111, 2056, 2429, 2000, 1996, 2940, 2045, 2024, 9390, 2040, 2057, 2215, 2000, 3713, 2000, 2004, 1037, 2765, 1997, 2070, 3522, 8973, 17186, 2091, 2005, 2678, 4905, 2236, 4388, 9111, 2409, 12060, 7483, 2008, 1996, 3425, 7640, 4812, 2046, 11851, 17789, 3235, 13108, 2577, 27946, 2003, 7552, 2372, 1997, 1996, 2047, 2259, 2103, 2473, 4929, 7415, 2666, 28095, 2015, 1999, 3638, 1997, 11851, 17789, 3235, 15885, 2006, 1996, 13082, 2604, 2004, 2027, 3233, 2362, 2006, 1996, 4084, 1997, 2103, 2534, 1999, 2047, 2259, 2076, 1037, 2739, 3034, 1037, 3204, 2044, 1996, 1026, 16371, 2213, 1028, 2095, 2214, 21153, 3516, 6319, 2001, 2915, 1998, 2730, 2011, 5101, 3422, 2386, 2577, 27946, 27946, 1998, 2010, 2522, 9517, 2123, 2225, 2187, 2831, 2076, 1037, 28290, 2006, 1996, 2034, 2154, 1997, 6467, 4989, 1999, 27946, 2015, 4028, 3979, 2197, 2621, 27946, 2001, 4821, 18538, 1997, 1996, 4735, 5571, 2021, 2079, 3501, 2071, 2145, 3288, 2976, 2942, 5571, 2114, 2032, 27946, 2040, 2003, 6696, 2001, 6951, 2007, 2010, 5101, 3422, 1999, 21153, 3516, 2043, 2002, 2018, 1996, 6497, 2039, 2007, 3235, 2008, 4504, 1999, 1996, 1026, 16371, 2213, 1028, 2095, 2214, 3060, 4841, 2331, 2002, 2001, 18538, 1997, 4028, 5571, 2062, 2084, 1037, 2095, 3283, 1999, 1037, 4735, 2457, 1996, 2533, 1997, 3425, 2071, 3288, 2049, 2219, 2275, 1997, 2976, 2942, 5571, 2114, 27946, 2174, 1998, 2049, 2145, 2559, 2000, 2241, 2006, 13304, 7928, 9432, 19867, 2506, 3037, 1999, 27946, 2003, 5866, 2004, 1996, 2533, 5373, 14730, 2049, 27050, 2306, 1037, 3043, 1997, 2706, 1996, 2940, 2758, 1996, 4905, 2236, 2134, 2102, 3749, 2151, 3176, 4751, 2006, 1996, 6867, 4812, 7483, 2107, 2004, 2043, 2009, 3488, 2000, 10236, 2009, 2039, 2043, 9111, 2001, 2356, 2055, 27946, 2197, 2281, 1996, 2406, 2015, 2708, 2375, 7285, 2961, 2134, 2102, 2507, 1037, 15764, 2203, 3058, 2005, 1996, 15113, 2593, 2021, 3264, 2008, 1037, 6937, 2112, 2001, 10395, 1999, 1996, 2553, 2008, 2001, 2699, 10047, 2025, 2469, 3599, 2129, 2172, 2936, 2008, 2097, 2202, 2021, 2057, 2097, 2131, 2000, 1037, 2391, 2073, 2057, 2024, 2583, 2000, 2191, 1037, 9128, 2002, 2056, 9111, 2001, 8781, 2153, 2055, 1996, 3235, 5008, 2006, 9432, 2012, 1037, 2811, 3034, 13856, 1037, 2079, 3501, 4812, 3141, 2000, 1996, 5008, 1997, 2178, 3060, 2137, 9458, 2745, 2829, 1999, 2008, 2553, 9111, 2056, 2079, 3501, 2097, 3319, 1996, 1996, 11262, 5284, 2610, 2486, 2000, 5646, 2065, 2049, 3738, 2031, 1037, 5418, 1998, 3218, 1997, 9147, 1996, 15113, 4076, 1037, 3204, 1997, 2942, 16591, 1999, 1996, 2358, 3434, 7575, 2044, 2028, 1997, 1996, 3262, 2304, 4865, 2317, 2610, 3738, 9382, 2915, 1998, 2730, 1026, 16371, 2213, 1028, 2095, 2214, 2829, 13240, 2155, 2003, 2108, 3421, 2011, 1996, 2168, 2942, 2916, 5160, 2040, 3421]\n",
      "[[1996, 2533, 1997, 3425, 2089, 2145, 3288, 2942, 5571, 2114, 11851, 17789, 3235, 13108, 2577, 27946, 4905, 2236, 4388, 9111, 3936, 2006, 9432], [9111, 2409, 12060, 2006, 9432, 2008, 2079, 3501, 2018, 2025, 5531, 2049, 4812, 2046, 1996, 2337, 1026, 16371, 2213, 1028, 1026, 16371, 2213, 1028, 5043, 1998, 1996, 3043, 2003, 7552], [2045, 2024, 3161, 4084, 2008, 2057, 2024, 2145, 1999, 1996, 2832, 1997, 2635, 9111, 2056, 2429, 2000, 1996, 2940], [2045, 2024, 9390, 2040, 2057, 2215, 2000, 3713, 2000, 2004, 1037, 2765, 1997, 2070, 3522, 8973], [17186, 2091, 2005, 2678, 4905, 2236, 4388, 9111, 2409, 12060, 7483, 2008, 1996, 3425, 7640, 4812, 2046, 11851, 17789, 3235, 13108, 2577, 27946, 2003, 7552, 2372, 1997, 1996, 2047, 2259, 2103, 2473, 4929, 7415, 2666, 28095, 2015, 1999, 3638, 1997, 11851, 17789, 3235, 15885, 2006, 1996, 13082, 2604, 2004, 2027, 3233, 2362, 2006, 1996, 4084, 1997, 2103, 2534, 1999, 2047, 2259, 2076, 1037, 2739, 3034, 1037, 3204, 2044, 1996, 1026, 16371, 2213, 1028, 2095, 2214, 21153, 3516, 6319, 2001, 2915, 1998, 2730, 2011, 5101, 3422, 2386, 2577, 27946, 27946, 1998, 2010, 2522, 9517, 2123, 2225, 2187, 2831, 2076, 1037, 28290, 2006, 1996, 2034, 2154, 1997, 6467, 4989, 1999, 27946, 2015, 4028, 3979, 2197, 2621], [27946, 2001, 4821, 18538, 1997, 1996, 4735, 5571, 2021, 2079, 3501, 2071, 2145, 3288, 2976, 2942, 5571, 2114, 2032, 27946, 2040, 2003, 6696, 2001, 6951, 2007, 2010, 5101, 3422, 1999, 21153, 3516, 2043, 2002, 2018, 1996, 6497, 2039, 2007, 3235, 2008, 4504, 1999, 1996, 1026, 16371, 2213, 1028, 2095, 2214, 3060, 4841, 2331], [2002, 2001, 18538, 1997, 4028, 5571, 2062, 2084, 1037, 2095, 3283, 1999, 1037, 4735, 2457], [1996, 2533, 1997, 3425, 2071, 3288, 2049, 2219, 2275, 1997, 2976, 2942, 5571, 2114, 27946, 2174, 1998, 2049, 2145, 2559, 2000, 2241, 2006, 13304, 7928, 9432], [19867, 2506, 3037, 1999, 27946, 2003, 5866, 2004, 1996, 2533, 5373, 14730, 2049, 27050, 2306, 1037, 3043, 1997, 2706], [1996, 2940, 2758, 1996, 4905, 2236, 2134, 2102, 3749, 2151, 3176, 4751, 2006, 1996, 6867, 4812, 7483, 2107, 2004, 2043, 2009, 3488, 2000, 10236, 2009, 2039], [2043, 9111, 2001, 2356, 2055, 27946, 2197, 2281, 1996, 2406, 2015, 2708, 2375, 7285, 2961, 2134, 2102, 2507, 1037, 15764, 2203, 3058, 2005, 1996, 15113, 2593, 2021, 3264, 2008, 1037, 6937, 2112, 2001, 10395, 1999, 1996, 2553, 2008, 2001, 2699], [10047, 2025, 2469, 3599, 2129, 2172, 2936, 2008, 2097, 2202, 2021, 2057, 2097, 2131, 2000, 1037, 2391, 2073, 2057, 2024, 2583, 2000, 2191, 1037, 9128, 2002, 2056], [9111, 2001, 8781, 2153, 2055, 1996, 3235, 5008, 2006, 9432, 2012, 1037, 2811, 3034, 13856, 1037, 2079, 3501, 4812, 3141, 2000, 1996, 5008, 1997, 2178, 3060, 2137, 9458, 2745, 2829], [1999, 2008, 2553, 9111, 2056, 2079, 3501, 2097, 3319, 1996, 1996, 11262, 5284, 2610, 2486, 2000, 5646, 2065, 2049, 3738, 2031, 1037, 5418, 1998, 3218, 1997, 9147], [1996, 15113, 4076, 1037, 3204, 1997, 2942, 16591, 1999, 1996, 2358, 3434, 7575, 2044, 2028, 1997, 1996, 3262, 2304, 4865, 2317, 2610, 3738, 9382, 2915, 1998, 2730, 1026, 16371, 2213, 1028, 2095, 2214, 2829], [13240, 2155, 2003, 2108, 3421, 2011, 1996, 2168, 2942, 2916, 5160, 2040, 3421, 1996, 11851, 17789, 19953, 3008, 25353, 23736, 2532, 17049, 1998, 10555, 3235, 2044, 2002, 2097, 2001, 2730, 2048, 1998, 1037, 2431, 2086, 3283], [17049, 1998, 3235, 2031, 2036, 3253, 2037, 3167, 2490, 2000, 13240, 2155], [1996, 3940, 6158, 2000, 5284, 2197, 3204, 2000, 3693, 1996, 10181, 13496, 3008, 2012, 2358, 3434, 3521, 14081, 8320, 2019, 3296, 2724, 6461, 2012, 4566, 3282, 4808]]\n",
      "[2114, 3288, 2942, 5160, 5571, 2079, 3501, 2114, 2079, 3501, 2145, 2577, 27946, 2145, 3288, 2976, 2145, 3421, 5571, 2942, 27946, 3421, 2071, 2916, 3008]\n"
     ]
    }
   ],
   "source": [
    "def preprocessing_fn(x, tokenizer):\n",
    "    x[\"summary_ids\"] = tokenizer(\n",
    "        x[\"summary\"],\n",
    "        add_special_tokens = False,\n",
    "        truncation = True,\n",
    "        max_length = 512,\n",
    "        padding = False,\n",
    "        return_attention_mask = True,\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    x[\"text_ids\"] = tokenizer(\n",
    "        x[\"text\"],\n",
    "        add_special_tokens = False,\n",
    "        truncation = True,\n",
    "        max_length = 512,\n",
    "        padding = False,\n",
    "        return_attention_mask = True,\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    x[\"sentences_ids\"] = tokenizer(\n",
    "        x[\"sentences\"],\n",
    "        add_special_tokens = False,\n",
    "        truncation = True,\n",
    "        max_length = 512,\n",
    "        padding = False,\n",
    "        return_attention_mask = True,\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    x[\"random_summary_ids\"] = tokenizer(\n",
    "        x[\"random_summary\"],\n",
    "        add_special_tokens = False,\n",
    "        truncation = True,\n",
    "        max_length = 512,\n",
    "        padding = False,\n",
    "        return_attention_mask = True,\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    return x\n",
    "\n",
    "splitted_dataset = dataset.select_columns([\"summary\", \"text\", \"sentences\", \"random_summary\"])\n",
    "\n",
    "word_lengths = get_word_lengths(splitted_dataset, tokenizer)\n",
    "\n",
    "# Tokenize the dataset\n",
    "splitted_dataset = splitted_dataset.map(\n",
    "    preprocessing_fn, tokenizer = tokenizer\n",
    ")\n",
    "\n",
    "# Remove useless columns\n",
    "splitted_dataset = splitted_dataset.select_columns([\"summary_ids\", \"text_ids\", \"sentences_ids\", \"random_summary_ids\"])\n",
    "print(splitted_dataset.__getitem__(0)[0]) # summary\n",
    "print(splitted_dataset.__getitem__(0)[1]) # text\n",
    "print(splitted_dataset.__getitem__(0)[2]) # sentences\n",
    "print(splitted_dataset.__getitem__(0)[3]) # random_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "871c9b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Get the tokenized sequences for each item in the batch\n",
    "    text_ids_batch = [torch.tensor(item[1], dtype = torch.int) for item in batch]\n",
    "    summary_ids_batch = [torch.tensor(item[0], dtype = torch.int) for item in batch]\n",
    "    sentences_ids_batch = [\n",
    "        [torch.tensor(sentence, dtype = torch.int) for sentence in item[2]]\n",
    "        for item in batch\n",
    "    ]\n",
    "\n",
    "    # Pad sequences to the maximum length in the batch\n",
    "    padded_text_ids = pad_sequence([torch.cat([item, torch.zeros(max(0, 512 - len(item)))]) for item in text_ids_batch], batch_first = True, padding_value = 0)\n",
    "    padded_summary_ids = pad_sequence([torch.cat([item, torch.zeros(max(0, 512 - len(item)))]) for item in summary_ids_batch], batch_first = True, padding_value = 0)\n",
    "    padded_sentences_ids = [\n",
    "        pad_sequence(\n",
    "            [torch.cat([sentence, torch.zeros(max(0, 512 - len(sentence)), dtype = torch.int)]) for sentence in item],\n",
    "            batch_first = True,\n",
    "            padding_value = 0\n",
    "        )\n",
    "        for item in sentences_ids_batch\n",
    "    ]\n",
    "\n",
    "    return {\"text_ids\": padded_text_ids, \"summary_ids\": padded_summary_ids, \"sentences_ids\": padded_sentences_ids}\n",
    "\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(splitted_dataset, batch_size = batch_size, collate_fn = collate_fn)\n",
    "\n",
    "epochs = 3\n",
    "def training(summary, text, model, epochs = 10):\n",
    "    model_copy = copy.deepcopy(model)\n",
    "    model_copy.train()\n",
    "\n",
    "    summary = summary.unsqueeze(0)\n",
    "    text = text.unsqueeze(0)\n",
    "    if summary.size(1) != text.size(1):\n",
    "        raise RuntimeError(\"Sizes along the sequence length dimension must match.\")\n",
    "    \n",
    "    for epochs in range(epochs):\n",
    "        whole_input = torch.cat((summary, text), dim = 0).long()\n",
    "        outputs = model_copy(whole_input, labels = whole_input)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return model_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "82de7dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_sentence(sentence, i, M, word_lengths):\n",
    "    tokenized_sentence = []\n",
    "    masked_token_ids = []\n",
    "\n",
    "    for j in range(len(sentence)):\n",
    "        if (j - i) % M == 0 and sentence[j].item() in word_lengths and word_lengths[sentence[j].item()]:\n",
    "            tokenized_sentence.append(tokenizer.mask_token_id) # 103\n",
    "            masked_token_ids.append(j)\n",
    "        else:\n",
    "            tokenized_sentence.append(sentence[j].item())\n",
    "            \n",
    "    tokenized_sentence= torch.tensor(tokenized_sentence)\n",
    "    \n",
    "    return tokenized_sentence, masked_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "fe719f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_BLANC_help(sentences, model, model_tuned, p_mask = 0.15):\n",
    "    S = [[0, 0], [0, 0]]\n",
    "    M = int(1/p_mask)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for i in range(M):\n",
    "            masked_sentence, masked_tokens_ids = mask_sentence(sentence, i, M, word_lengths)\n",
    "            masked_sentence = torch.tensor(masked_sentence).view(1, -1).long()\n",
    "            sentence = sentence.long()\n",
    "\n",
    "            out_base = model(masked_sentence, labels = sentence).logits\n",
    "            out_help = model_tuned(masked_sentence, labels = sentence).logits\n",
    "            # print(out_base.shape)\n",
    "            # print(out_base[0][0].shape)\n",
    "\n",
    "            for j in masked_tokens_ids:\n",
    "                predicted_token_model = torch.argmax(out_base[0][j])\n",
    "                predicted_token_model_tuned = torch.argmax(out_help[0][j])\n",
    "                \n",
    "                k = int(predicted_token_model == sentence[j])\n",
    "                m = int(predicted_token_model_tuned == sentence[j])\n",
    "                S[k][m] += 1\n",
    "        break\n",
    "    try:\n",
    "      B = (S[0][1] - S[1][0]) / (S[0][0] + S[1][1] + S[0][1] + S[1][0])\n",
    "    except ZeroDivisionError:\n",
    "      B = 0.0\n",
    "    \n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "99b978b0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary 0 of batch\n"
     ]
    }
   ],
   "source": [
    "def blanc_tune(summary, text, model, p_mask = 0.15, N = 10, epochs = 10):\n",
    "    N_summary = len(summary[:summary.tolist().index(0)])\n",
    "    N_mask = int(N_summary*p_mask)\n",
    "    set_tune = pd.DataFrame(columns = ['summary', 'text'])\n",
    "\n",
    "    for j in range(0, N):\n",
    "        pos = [i for i, token in enumerate(summary.tolist()) if token in word_lengths and word_lengths[token]]\n",
    "        random.shuffle(pos)\n",
    "        # print(len(pos), pos)\n",
    "        # print(N_mask)\n",
    "        while len(pos) != 0:\n",
    "            masked_summary = summary.tolist().copy()\n",
    "            for pos_to_mask in pos[:N_mask]:\n",
    "                masked_summary[pos_to_mask] = '[MASK]'\n",
    "                set_tune.loc[set_tune.shape[0]] = [masked_summary, text]\n",
    "            pos = pos[N_mask:]\n",
    "\n",
    "    model_tuned = training(summary, text, model, epochs)\n",
    "    print('\\n')      \n",
    "    return model_tuned\n",
    "\n",
    "def blanc_tune_batch(batch, model, p_mask = 0.15, N = 10, epochs = 10):\n",
    "    batch_tuned_models = []\n",
    "    batch_accuracies = []\n",
    "    \n",
    "    i = 0\n",
    "    for summary, text, sentences in zip(batch['summary_ids'], batch['text_ids'], batch['sentences_ids']):\n",
    "        print(f\"Summary {i} of batch\")\n",
    "        i += 1\n",
    "        start_time = time.time()\n",
    "        model_tuned = blanc_tune(summary, text, model, p_mask, N, epochs)\n",
    "        batch_tuned_models.append(model_tuned)\n",
    "        accuracy = modified_BLANC_help(sentences, model, model_tuned)\n",
    "        print(accuracy)\n",
    "        end_time = time.time()\n",
    "        batch_accuracies.append(accuracy)\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Elapsed Time: {elapsed_time} seconds\")\n",
    "\n",
    "    return batch_tuned_models, batch_accuracies\n",
    "\n",
    "for batch in dataloader:\n",
    "    models_tuned, text_accuracies = blanc_tune_batch(batch, model, epochs = epochs)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "collabfiltering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
