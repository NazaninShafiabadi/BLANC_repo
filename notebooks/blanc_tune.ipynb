{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5d9f8b7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random as rd\n",
    "from transformers import BertTokenizer, BertForMaskedLM, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b872e196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Users\\Liora\\anaconda3\\envs\\collabfiltering\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_json('./datasets/DailyNews_300.json')\n",
    "print(data.shape)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "397b468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.features = self.dataset.columns\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.dataset.iloc[idx, 0], self.dataset.iloc[idx, 1])\n",
    "    \n",
    "    def map(self, preprocessing_fn, **kwargs):\n",
    "        return CustomDataset(self.dataset.apply(lambda x: preprocessing_fn(x, **kwargs), axis = 1))\n",
    "    \n",
    "    def select_columns(self, columns):\n",
    "        new_dataset = self.dataset[columns] \n",
    "        return CustomDataset(new_dataset)\n",
    "    \n",
    "dataset = CustomDataset(data)\n",
    "# print(dataset.__getitem__(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12178dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Former England defender Gary Neville suggested Gareth Southgate \\' s squad had done more than could have been expected of them at this World Cup.\\nIvan Perisic and Mario Mandzukic goals earned Croatia a date with France after a 2 - 1 extra - time win in Moscow.\\nNeville : \" This team has taken us to a place we never imagined we could get \"\\nEngland \\' s Dejan Lovren and Domagoj Vida were sent off in the second half.\\nDefender Rio Ferdinand says experience was a telling factor in the England squad.\\nEngland face France in the World Cup final in Brazil.', 'Mario Mandzukic pounces to fire the ball past Jordan Pickford and put Croatia into the World Cup final. Photo: Reuters Independent.ie \\n \\nFormer England defender Gary Neville suggested Gareth Southgate\\'s squad had done more than could have been expected of them at this World Cup as they bowed out with a semi-final defeat against Croatia. \\n \\nhttps://www.independent.ie/sport/soccer/world-cup-2018/gary-neville-salutes-englands-overachievers-as-alan-shearer-and-rio-ferdinand-give-their-verdicts-37108667.html \\n \\nhttps://www.independent.ie/incoming/article37108634.ece/7571a/AUTOCROP/h342/52Man1.jpg \\n   Email     \\nFormer England defender Gary Neville suggested Gareth Southgate\\'s squad had done more than could have been expected of them at this World Cup as they bowed out with a semi-final defeat against Croatia. \\n  \\nA jaded England faded after had Kieran Trippier fired England ahead after just five minutes with a superb free-kick, with goals from Ivan Perisic and Mario Mandzukic earned Croatia a date with France after a 2-1 extra-time win in Moscow. \\n \\n\"The first thing to say is this team has taken us to a place we never imagined we could get,\" said Neville. \\n \\n\"I said before the tournament England never overachieve, this team has overachieved getting to the semi-final and they can all be absolutely proud. \\n \\n\"They have taken the nation with them. You see all the fans out in the park to see them. I never thought I\\'d see scenes like that in this World Cup. \\n \\n\"They can be absolutely proud. We did say before the game these lads have to take this opportunity because it may never come again. \\n \\n\"All I hope as I look at them now crying, disappointed, upset is they get another opportunity as good as the one they\\'ve just had.\" \\n \\n\"They\\'re the youngest team in the tournament. I hope they\\'re in that position again, 1-0 up with 20 minutes to go with the opportunity of getting into a World Cup final.\" \\n  Croatia\\'s Dejan Lovren and Domagoj Vida celebrate after the match. Photo: Reuters   \\nFormer England defender Rio Ferdinand believes the youthful nature of England\\'s squad proved to be their undoing in the semi-final. \\n \\n\"I think experience was a telling factor today,\" Ferdinand told the BBC. \"This Croatian team had medals all over the squad but now is not the time to knitpick. This bunch of players have brought the nation together and the fans haven\\'t sung like this for years. \\n \\n\"We wanted this team to come out with an identity and a philosophy of playing, and I think they have done that. Where we were sat in the stadium, people around us were impressed with England\\'s football and their ability to play out from the back. \\n \\n\"Kieran Trippier\\'s delivery of the ball was brilliant and what a tournament he has had. There were question marks over Jordan Pickford but he was the best choice in goal and this team can develop into something very strong. I have been an admirer of John Stones for a long time, and he defended unbelievably today. We have got a very inexperienced young squad but they have grown and will continue to do so.\" \\n \\nEx-England striker Alan Shearer also tried to find the positives amid England\\'s anguish. \\n \\n\"We said before the tournament that if they gave absolutely everything in every game, everyone at home would be happy,\" said Shearer. \\n \\n\"England got off to a very good start and dominated the first 45 minutes but Croatia stepped 10 yards further forward in the second half. \\n \\n\"Luka Modric bossed the midfield and we could not get near him with his deft touches and passes, he was superb and a man of the match performance, a vital cog for them. We lacked experience to get someone on him and stop him. Croatia did not panic when they fell behind. \\n \\n\"Kieran Trippier was brilliant in the tournament and so was Jordan Pickford, especially with his delivery. Harry Kane was a bit quiet in the knockout games and he had a chance that he could not put away \\n \\n\"So many positives to take out of the tournament, we did not expect to get to the quarters and will go home after this without heads held high. Finishing third would be excellent. Well done guys, you have given us plenty to shout about.\" \\n \\nOnline Editors')\n",
      "([2280, 2563, 8291, 5639, 14871, 4081, 20243, 2148, 5867, 1005, 1055, 4686, 2018, 2589, 2062, 2084, 2071, 2031, 2042, 3517, 1997, 2068, 2012, 2023, 2088, 2452, 1012, 7332, 2566, 17417, 2278, 1998, 7986, 2158, 2094, 24015, 2278, 3289, 3687, 8097, 1037, 3058, 2007, 2605, 2044, 1037, 1016, 1011, 1015, 4469, 1011, 2051, 2663, 1999, 4924, 1012, 14871, 1024, 1000, 2023, 2136, 2038, 2579, 2149, 2000, 1037, 2173, 2057, 2196, 8078, 2057, 2071, 2131, 1000, 2563, 1005, 1055, 2139, 8405, 8840, 12229, 2078, 1998, 14383, 23692, 3501, 19830, 2020, 2741, 2125, 1999, 1996, 2117, 2431, 1012, 8291, 5673, 9684, 2758, 3325, 2001, 1037, 4129, 5387, 1999, 1996, 2563, 4686, 1012, 2563, 2227, 2605, 1999, 1996, 2088, 2452, 2345, 1999, 4380, 1012], [7986, 2158, 2094, 24015, 2278, 13433, 17457, 2015, 2000, 2543, 1996, 3608, 2627, 5207, 4060, 3877, 1998, 2404, 8097, 2046, 1996, 2088, 2452, 2345, 1012, 6302, 1024, 26665, 2981, 1012, 29464, 2280, 2563, 8291, 5639, 14871, 4081, 20243, 2148, 5867, 1005, 1055, 4686, 2018, 2589, 2062, 2084, 2071, 2031, 2042, 3517, 1997, 2068, 2012, 2023, 2088, 2452, 2004, 2027, 11489, 2041, 2007, 1037, 4100, 1011, 2345, 4154, 2114, 8097, 1012, 16770, 1024, 1013, 1013, 7479, 1012, 2981, 1012, 29464, 1013, 4368, 1013, 4715, 1013, 2088, 1011, 2452, 1011, 2760, 1013, 5639, 1011, 14871, 1011, 17664, 2015, 1011, 2563, 2015, 1011, 2058, 21046, 22507, 2015, 1011, 2004, 1011, 5070, 1011, 18330, 2121, 1011, 1998, 1011, 5673, 1011, 9684, 1011, 2507, 1011, 2037, 1011, 14392, 2015, 1011, 4261, 10790, 20842, 2575, 2581, 1012, 16129, 16770, 1024, 1013, 1013, 7479, 1012, 2981, 1012, 29464, 1013, 14932, 1013, 3720, 24434, 10790, 20842, 22022, 1012, 14925, 2063, 1013, 4293, 2581, 2487, 2050, 1013, 8285, 26775, 7361, 1013, 1044, 22022, 2475, 1013, 4720, 2386, 2487, 1012, 16545, 2290, 10373, 2280, 2563, 8291, 5639, 14871, 4081, 20243, 2148, 5867, 1005, 1055, 4686, 2018, 2589, 2062, 2084, 2071, 2031, 2042, 3517, 1997, 2068, 2012, 2023, 2088, 2452, 2004, 2027, 11489, 2041, 2007, 1037, 4100, 1011, 2345, 4154, 2114, 8097, 1012, 1037, 12323, 2094, 2563, 8105, 2044, 2018, 11382, 23169, 4440, 14756, 2099, 5045, 2563, 3805, 2044, 2074, 2274, 2781, 2007, 1037, 21688, 2489, 1011, 5926, 1010, 2007, 3289, 2013, 7332, 2566, 17417, 2278, 1998, 7986, 2158, 2094, 24015, 2278, 3687, 8097, 1037, 3058, 2007, 2605, 2044, 1037, 1016, 1011, 1015, 4469, 1011, 2051, 2663, 1999, 4924, 1012, 1000, 1996, 2034, 2518, 2000, 2360, 2003, 2023, 2136, 2038, 2579, 2149, 2000, 1037, 2173, 2057, 2196, 8078, 2057, 2071, 2131, 1010, 1000, 2056, 14871, 1012, 1000, 1045, 2056, 2077, 1996, 2977, 2563, 2196, 2058, 21046, 18697, 1010, 2023, 2136, 2038, 2058, 21046, 18697, 2094, 2893, 2000, 1996, 4100, 1011, 2345, 1998, 2027, 2064, 2035, 2022, 7078, 7098, 1012, 1000, 2027, 2031, 2579, 1996, 3842, 2007, 2068, 1012, 2017, 2156, 2035, 1996, 4599, 2041, 1999, 1996, 2380, 2000, 2156, 2068, 1012, 1045, 2196, 2245, 1045, 1005, 1040, 2156, 5019, 2066, 2008, 1999, 2023, 2088, 2452, 1012, 1000, 2027, 2064, 2022, 7078, 7098, 1012, 2057, 2106, 2360, 2077, 1996, 2208, 2122, 29126, 2031, 2000, 2202, 2023, 4495, 2138, 2009, 2089, 2196, 2272, 2153, 1012, 1000, 2035, 1045, 3246, 2004, 1045, 2298, 2012, 2068, 2085, 6933, 1010, 9364, 1010, 6314, 2003, 2027, 2131, 2178, 4495, 2004, 2204, 2004, 1996, 2028, 2027, 1005, 2310, 2074, 2018, 1012, 1000, 1000, 2027, 1005, 2128, 1996, 6587, 2136, 1999, 1996, 2977, 1012, 1045, 3246, 2027, 1005, 2128, 1999, 2008, 2597, 2153, 1010, 1015, 1011, 1014, 2039, 2007, 2322, 2781, 2000, 2175, 2007, 1996, 4495, 1997, 2893, 2046, 1037, 2088, 2452, 2345, 1012, 1000, 8097, 1005, 1055, 2139, 8405, 8840, 12229, 2078, 1998, 14383, 23692, 3501, 19830, 8439, 2044, 1996, 2674, 1012, 6302, 1024, 26665, 2280, 2563, 8291, 5673, 9684, 7164, 1996, 22446, 3267, 1997, 2563, 1005, 1055, 4686, 4928, 2000, 2022, 2037, 25672, 2075, 1999, 1996, 4100, 1011, 2345])\n"
     ]
    }
   ],
   "source": [
    "def preprocessing_fn(x, tokenizer):\n",
    "    x[\"summary_ids\"] = tokenizer(\n",
    "        x[\"summary\"],\n",
    "        add_special_tokens = False,\n",
    "        truncation = True,\n",
    "        max_length = 512,\n",
    "        padding = False,\n",
    "        return_attention_mask = False,\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    x[\"text_ids\"] = tokenizer(\n",
    "        x[\"text\"],\n",
    "        add_special_tokens = False,\n",
    "        truncation = True,\n",
    "        max_length = 512,\n",
    "        padding = False,\n",
    "        return_attention_mask = False,\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    return x\n",
    "\n",
    "splitted_dataset = dataset.select_columns([\"summary\", \"text\"])\n",
    "# print(splitted_dataset.__getitem__(0))\n",
    "\n",
    "# Tokenize the dataset\n",
    "splitted_dataset = splitted_dataset.map(\n",
    "    preprocessing_fn, tokenizer = tokenizer\n",
    ")\n",
    "print(splitted_dataset.__getitem__(0))\n",
    "\n",
    "# Remove useless columns\n",
    "splitted_dataset = splitted_dataset.select_columns([\"summary_ids\", \"text_ids\"])\n",
    "print(splitted_dataset.__getitem__(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "871c9b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Get the tokenized sequences for each item in the batch\n",
    "    text_ids_batch = [torch.tensor(item[1], dtype = torch.int) for item in batch]\n",
    "    summary_ids_batch = [torch.tensor(item[0], dtype = torch.int) for item in batch]\n",
    "\n",
    "    # Pad sequences to the maximum length in the batch\n",
    "    padded_text_ids = pad_sequence(text_ids_batch, batch_first = True, padding_value = 0)\n",
    "    padded_summary_ids = pad_sequence(summary_ids_batch, batch_first = True, padding_value = 0)\n",
    "\n",
    "    return {\"text_ids\": padded_text_ids, \"summary_ids\": padded_summary_ids}\n",
    "\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(splitted_dataset, batch_size = batch_size, collate_fn = collate_fn)\n",
    "\n",
    "epochs = 3\n",
    "def training(summary, text, model, epochs = 10):\n",
    "    model_copy = model.copy()\n",
    "    model_copy.train()\n",
    "    for epochs in range(epochs):\n",
    "        outputs = model_copy(summary, text)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return model_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "99b978b0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text_ids': tensor([[ 7986,  2158,  2094,  ...,  4100,  1011,  2345],\n",
      "        [ 2005,  1996,  3822,  ...,  2606, 11917,  3737],\n",
      "        [ 6264,  1517,  1996,  ...,  2005,  1996,  2110],\n",
      "        ...,\n",
      "        [ 5655, 16216, 19020,  ..., 19723, 24454,  3686],\n",
      "        [ 1996,  4501,  2966,  ..., 12763,  3378,  2007],\n",
      "        [ 2062,  2084,  1002,  ...,  2095,  1011,  2214]], dtype=torch.int32), 'summary_ids': tensor([[ 2280,  2563,  8291,  ...,     0,     0,     0],\n",
      "        [ 1048,  1005, 10848,  ...,     0,     0,     0],\n",
      "        [ 2655, 21293,  2100,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 5655, 16216, 19020,  ...,     0,     0,     0],\n",
      "        [ 2151,  6926,  1010,  ...,     0,     0,     0],\n",
      "        [ 3533,  7226,  2368,  ...,     0,     0,     0]], dtype=torch.int32)}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[1;32m---> 30\u001b[0m     tuned_models \u001b[38;5;241m=\u001b[39m \u001b[43mblanc_tune_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[53], line 23\u001b[0m, in \u001b[0;36mblanc_tune_batch\u001b[1;34m(batch, model, p_mask, l_min, N)\u001b[0m\n\u001b[0;32m     21\u001b[0m batch_tuned_models \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m summary, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m---> 23\u001b[0m     tuned_model \u001b[38;5;241m=\u001b[39m \u001b[43mblanc_tune\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     batch_tuned_models\u001b[38;5;241m.\u001b[39mappend(tuned_model)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch_tuned_models\n",
      "Cell \u001b[1;32mIn[53], line 4\u001b[0m, in \u001b[0;36mblanc_tune\u001b[1;34m(summary, text, model, p_mask, l_min, N, epochs)\u001b[0m\n\u001b[0;32m      2\u001b[0m N_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(summary)\n\u001b[0;32m      3\u001b[0m N_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(N_summary\u001b[38;5;241m*\u001b[39mp_mask)\n\u001b[1;32m----> 4\u001b[0m summary_detokenized \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary_detokenized)\n\u001b[0;32m      7\u001b[0m set_tune \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Liora\\anaconda3\\envs\\collabfiltering\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3548\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   3527\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3528\u001b[0m \u001b[38;5;124;03mConverts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\u001b[39;00m\n\u001b[0;32m   3529\u001b[0m \u001b[38;5;124;03mtokens and clean up tokenization spaces.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3545\u001b[0m \u001b[38;5;124;03m    `str`: The decoded sentence.\u001b[39;00m\n\u001b[0;32m   3546\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3547\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m-> 3548\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m \u001b[43mto_py_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[0;32m   3551\u001b[0m     token_ids\u001b[38;5;241m=\u001b[39mtoken_ids,\n\u001b[0;32m   3552\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   3553\u001b[0m     clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   3554\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3555\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Liora\\anaconda3\\envs\\collabfiltering\\lib\\site-packages\\transformers\\utils\\generic.py:206\u001b[0m, in \u001b[0;36mto_py_obj\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: to_py_obj(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [to_py_obj(o) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m obj]\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_tf_tensor(obj):\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\Liora\\anaconda3\\envs\\collabfiltering\\lib\\site-packages\\transformers\\utils\\generic.py:206\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: to_py_obj(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mto_py_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m obj]\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_tf_tensor(obj):\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\Liora\\anaconda3\\envs\\collabfiltering\\lib\\site-packages\\transformers\\utils\\generic.py:207\u001b[0m, in \u001b[0;36mto_py_obj\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [to_py_obj(o) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m obj]\n\u001b[1;32m--> 207\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mis_tf_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_torch_tensor(obj):\n",
      "File \u001b[1;32mc:\\Users\\Liora\\anaconda3\\envs\\collabfiltering\\lib\\site-packages\\transformers\\utils\\generic.py:166\u001b[0m, in \u001b[0;36mis_tf_tensor\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_tf_tensor\u001b[39m(x):\n\u001b[0;32m    163\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    Tests if `x` is a tensorflow tensor or not. Safe to call even if tensorflow is not installed.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_is_tensorflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Liora\\anaconda3\\envs\\collabfiltering\\lib\\site-packages\\transformers\\utils\\generic.py:159\u001b[0m, in \u001b[0;36m_is_tensorflow\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_is_tensorflow\u001b[39m(x):\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'Tensor'"
     ]
    }
   ],
   "source": [
    "def blanc_tune(summary, text, model, p_mask = 0.15, l_min = 4, N = 10, epochs = 10):\n",
    "    N_summary = len(summary)\n",
    "    N_mask = int(N_summary*p_mask)\n",
    "    summary_detokenized = tokenizer.decode(summary.numpy().tolist())\n",
    "    print(summary_detokenized)\n",
    "\n",
    "    set_tune = pd.DataFrame(columns = ['summary', 'text'])\n",
    "    for _ in range(1, N + 1):\n",
    "        pos = [i for i, word in enumerate() if len(word) >= l_min]\n",
    "        pos = rd.shuffle(pos)\n",
    "        while len(pos) != 0:\n",
    "            masked_summary = words_in_summary.copy()\n",
    "            for pos_to_mask in pos[:N_mask]:\n",
    "                masked_summary[pos_to_mask] = '<MASK>'\n",
    "                set_tune.loc[set_tune.shape[0]] = [masked_summary, text]\n",
    "    model_tuned = training(summary, text, model, epochs)\n",
    "            \n",
    "    return model_tuned\n",
    "\n",
    "def blanc_tune_batch(batch, model, p_mask = 0.15, l_min = 4, N = 10):\n",
    "    batch_tuned_models = []\n",
    "    for summary, text in zip(batch['summary_ids'], batch['text_ids']):\n",
    "        tuned_model = blanc_tune(summary, text, model, p_mask, l_min, N)\n",
    "        batch_tuned_models.append(tuned_model)\n",
    "\n",
    "    return batch_tuned_models\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    tuned_models = blanc_tune_batch(batch, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01ee7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "collabfiltering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
