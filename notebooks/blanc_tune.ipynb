{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a5d9f8b7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random as rd\n",
    "from transformers import BertTokenizer, BertForMaskedLM, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b872e196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Users\\Liora\\anaconda3\\envs\\collabfiltering\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_json('./datasets/DailyNews_300.json')\n",
    "print(data.shape)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397b468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.features = self.dataset.columns\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return f'Text : {self.dataset.iloc[idx, 3]}, \\nSummary : {self.dataset.iloc[idx, 2]}\\n'\n",
    "    \n",
    "    def map(self, preprocessing_fn, **kwargs):\n",
    "        return CustomDataset(self.dataset.apply(lambda x: preprocessing_fn(x, **kwargs), axis=1))\n",
    "    \n",
    "    def select_columns(self, columns):\n",
    "        return self.dataset[columns]\n",
    "    \n",
    "dataset = CustomDataset(data)\n",
    "# print(dataset.__getitem__(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "12178dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fn(x, tokenizer):\n",
    "    x[\"summary_ids\"] = tokenizer(\n",
    "        x[\"summary\"],\n",
    "        add_special_tokens=False,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False,\n",
    "        return_attention_mask=False,\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    x[\"text_ids\"] = tokenizer(\n",
    "        x[\"text\"],\n",
    "        add_special_tokens=False,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False,\n",
    "        return_attention_mask=False,\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    return x\n",
    "\n",
    "# Tokenize the dataset\n",
    "splitted_dataset = dataset.map(\n",
    "    preprocessing_fn, tokenizer=tokenizer\n",
    ")\n",
    "# print(splitted_dataset.features)\n",
    "\n",
    "# Remove useless columns\n",
    "splitted_dataset = splitted_dataset.select_columns([\"summary_ids\", \"text_ids\"])\n",
    "# print(splitted_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "871c9b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Get the tokenized sequences for each item in the batch\n",
    "    text_ids_batch = [torch.tensor(item[\"text_ids\"], dtype = torch.int) for item in batch]\n",
    "    summary_ids_batch = [torch.tensor(item[\"summary_ids\"], dtype = torch.int) for item in batch]\n",
    "\n",
    "    # Pad sequences to the maximum length in the batch\n",
    "    padded_text_ids = pad_sequence(text_ids_batch, batch_first = True, padding_value = 0)\n",
    "    padded_summary_ids = pad_sequence(summary_ids_batch, batch_first = True, padding_value = 0)\n",
    "\n",
    "    return {\"text_ids\": padded_text_ids, \"summary_ids\": padded_summary_ids}\n",
    "\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(splitted_dataset, batch_size = batch_size, collate_fn = collate_fn, shuffle = True)\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "def training(dataloader, model, epochs = 10):\n",
    "    model_copy = model.copy()\n",
    "\n",
    "    for epochs in range(epochs):\n",
    "        model_copy.train()\n",
    "        for batch in dataloader:\n",
    "            text_ids = batch[\"text_ids\"]\n",
    "            summary_ids = batch[\"summary_ids\"]\n",
    "\n",
    "            outputs = model_copy(text_ids, summary_ids)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    return model_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6d24d95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = DataLoader(dataset, batch_size = 32, shuffle = True)\n",
    "# print(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b978b0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# summary and text should be tonkenized beforehand\n",
    "tokenized_summary = tokenizer(summary, return_tensors='pt', max_length=600, truncation=True)\n",
    "tokenized_text = tokenizer(text, return_tensors='pt', max_length=600, truncation=True)\n",
    "\n",
    "def blanc_tune(summary, text, model, p_mask = 0.15, l_min = 4, N = 10):\n",
    "    words_in_summary = summary.split()\n",
    "    N_summary = len(words_in_summary)\n",
    "    N_mask = int(N_summary*p_mask)\n",
    "    set_tune = pd.DataFrame(columns = ['summary', 'text'])\n",
    "    for i in range(1, N + 1):\n",
    "        pos = [i for i, word in enumerate(words_in_summary) if len(word) >= l_min]\n",
    "        pos = rd.shuffle(pos)\n",
    "        while len(pos) != 0:\n",
    "            masked_summary = words_in_summary.copy()\n",
    "            for pos_to_mask in pos[:N_mask]:\n",
    "                masked_summary[pos_to_mask] = '<MASK>'\n",
    "                set_tune.loc[set_tune.shape[0]] = [masked_summary, text]\n",
    "    # add tuning of model (see below, from chatgpt, also look at homework 2)\n",
    "            \n",
    "    return\n",
    "\n",
    "model_tuned = blanc_tune(tokenized_summary, tokenized_text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaa9538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training loop:\n",
    "for epoch in range(3):  # Replace with the desired number of epochs\n",
    "    model.train()\n",
    "    for batch in cloze_dataloader:\n",
    "        inputs = tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True, return_special_tokens_mask=True)\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "        labels = tokenizer(batch['summary'], return_tensors='pt', padding=True, truncation=True)['input_ids'].to(device)\n",
    "\n",
    "        # Ensure that labels are masked only at the [MASK] token positions\n",
    "        labels[inputs['input_ids'] == tokenizer.mask_token_id] = -100\n",
    "\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f97eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Save the fine-tuned model\n",
    "model.save_pretrained('fine_tuned_bert_cloze_model')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "collabfiltering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
