{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# The models the authors used:\n",
    "from transformers import BertForMaskedLM, BertTokenizer, BertModel\n",
    "\n",
    "from blanc import BLANC_help_summary, BLANC_help_translation, add_results_to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "bert_checkpoint = \"bert-base-uncased\"\n",
    "bert_model = BertForMaskedLM.from_pretrained(bert_checkpoint).to(DEVICE)\n",
    "word_sim_model = BertModel.from_pretrained(bert_checkpoint).to(DEVICE)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_checkpoint, do_lower_case=True)\n",
    "\n",
    "mbert_checkpoint = \"bert-base-multilingual-uncased\"\n",
    "mbert_model = BertForMaskedLM.from_pretrained(mbert_checkpoint).to(DEVICE)\n",
    "mbert_tokenizer = BertTokenizer.from_pretrained(mbert_checkpoint, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLANC help for **summaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Datasets \"\"\"\n",
    "\n",
    "# cnn_dailymail_ds = load_dataset(\"cnn_dailymail\", '3.0.0', split='test')\n",
    "\n",
    "DailyNews_ds = load_dataset(\n",
    "    \"json\", data_files=\"../datasets/DailyNews_300.json\", split=\"train\"\n",
    ")\n",
    "DailyNews_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Preprocessing \"\"\"\n",
    "\n",
    "summaries = DailyNews_ds[\"summary\"]  # (List[str])\n",
    "texts = DailyNews_ds[\n",
    "    \"text\"\n",
    "]  # (List[str]) each string is a paragraph made of a few sentences\n",
    "\n",
    "# each text in texts is a list of sentences (each sentence is a string)\n",
    "texts = [sent_tokenize(text.strip()) for text in texts]  # List[List[str]]\n",
    "assert len(texts) == len(summaries) == 300\n",
    "\n",
    "tokenized_texts = [\n",
    "    [bert_tokenizer.tokenize(sentence) for sentence in text] for text in texts\n",
    "]  # List[List[List[str]]]\n",
    "tokenized_summaries = [\n",
    "    bert_tokenizer.tokenize(summary) for summary in summaries\n",
    "]  # [List[List[str]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Running the Program \"\"\"\n",
    "\n",
    "help_summary_scores = [\n",
    "    BLANC_help_summary(\n",
    "        text,\n",
    "        summary,\n",
    "        bert_model,\n",
    "        bert_tokenizer,\n",
    "        device=DEVICE,\n",
    "        word_sim_model=word_sim_model,\n",
    "    )\n",
    "    for summary, text in tqdm(zip(tokenized_summaries, tokenized_texts))\n",
    "]\n",
    "\n",
    "# save scores to results.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLANC help for **translations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Datasets \"\"\"\n",
    "\n",
    "# English - French\n",
    "en_fr_ds = load_dataset(\"news_commentary\", \"en-fr\", split=\"train\")\n",
    "\n",
    "# English - Persian (Farsi)\n",
    "en_fa_ds = load_dataset(\"persiannlp/parsinlu_translation_en_fa\", split=\"train\")\n",
    "\n",
    "# English - Persian (with annotator scores)\n",
    "en_fa_with_scores = pd.read_csv(\"/content/en-fa(0-55)_with_grades.csv\", index_col=0)\n",
    "en_fa_with_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Preprocessing (English - French)\"\"\"\n",
    "\n",
    "en_fr_ds = (\n",
    "    en_fr_ds.map(lambda example: example[\"translation\"])\n",
    "    .remove_columns([\"id\", \"translation\"])\n",
    "    .rename_column(\"en\", \"sentence\")\n",
    "    .rename_column(\"fr\", \"translation\")\n",
    "    .select(range(300))\n",
    ")\n",
    "\n",
    "# Tokenization\n",
    "en_fr_sentences = [\n",
    "    mbert_tokenizer.tokenize(sentence) for sentence in en_fr_ds[\"sentence\"]\n",
    "]  # (List[List[str]])\n",
    "\n",
    "en_fr_translations = [\n",
    "    mbert_tokenizer.tokenize(translation) for translation in en_fr_ds[\"translation\"]\n",
    "]  # (List[List[str]])\n",
    "\n",
    "\n",
    "\"\"\" Preprocessing (English - Persian (Farsi)) \"\"\"\n",
    "\n",
    "# Removing the 'category' column\n",
    "en_fa_ds = en_fa_ds.remove_columns([\"category\"])\n",
    "\n",
    "# Removing list encapsulation\n",
    "en_fa_ds = en_fa_ds.map(lambda example: {\"targets\": example[\"targets\"][0]}, num_proc=4)\n",
    "\n",
    "# Filtering out:\n",
    "# - rows with the '\\u200c' symbol,\n",
    "# - those where the length of either source or targets is less than a threshold\n",
    "# - Headlines (ending in 'Global Voices') --> because they are very short and the 'Global Voices' part is never translated\n",
    "length_threshold = 30\n",
    "filtered_en_fa_ds = en_fa_ds.filter(\n",
    "    lambda example: \"\\u200c\" not in example[\"targets\"]\n",
    "    and len(example[\"source\"]) >= length_threshold\n",
    "    and len(example[\"targets\"]) >= length_threshold\n",
    "    and \"Global Voices\" not in example[\"source\"],\n",
    "    num_proc=4,\n",
    ")\n",
    "\n",
    "en_fa_ds = (\n",
    "    filtered_en_fa_ds.rename_column(\"source\", \"sentence\")\n",
    "    .rename_column(\"targets\", \"translation\")\n",
    "    .select(range(300))\n",
    ")\n",
    "\n",
    "# Tokenization\n",
    "en_fa_sentences = [\n",
    "    mbert_tokenizer.tokenize(sentence) for sentence in en_fa_ds[\"sentence\"]\n",
    "]  # (List[List[str]])\n",
    "\n",
    "en_fa_translations = [\n",
    "    mbert_tokenizer.tokenize(translation) for translation in en_fa_ds[\"translation\"]\n",
    "]  # (List[List[str]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Running the Program (English - French)\"\"\"\n",
    "\n",
    "%%time\n",
    "help_en_fr_scores = [\n",
    "    BLANC_help_translation(\n",
    "        translation, sentence, mbert_model, mbert_tokenizer, device=DEVICE\n",
    "    )\n",
    "    for translation, sentence in tqdm(\n",
    "        zip(en_fr_translations, en_fr_sentences), total=len(en_fr_sentences)\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Running the Program (English - Persian)\"\"\"\n",
    "\n",
    "%%time\n",
    "help_en_fa_scores = [\n",
    "    BLANC_help(translation, sentence, mbert_model, mbert_tokenizer, device=DEVICE)\n",
    "    for translation, sentence in tqdm(\n",
    "        zip(en_fa_translations, en_fa_sentences), total=len(en_fa_sentences)\n",
    "    )\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
