{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# The models the authors used:\n",
    "from transformers import BertForMaskedLM, BertTokenizer, BertModel\n",
    "\n",
    "from blanc import BLANC_tune_translation, add_results_to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "bert_checkpoint = 'bert-base-uncased'\n",
    "model = BertForMaskedLM.from_pretrained(bert_checkpoint).to(DEVICE)\n",
    "word_sim_model = BertModel.from_pretrained(bert_checkpoint).to(DEVICE)\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_checkpoint, do_lower_case = True)\n",
    "\n",
    "mbert_checkpoint = 'bert-base-multilingual-uncased'\n",
    "mbert_model = BertForMaskedLM.from_pretrained(mbert_checkpoint).to(DEVICE)\n",
    "mbert_tokenizer = BertTokenizer.from_pretrained(mbert_checkpoint, do_lower_case = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLANC tune for **summaries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLANC tune for **translations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Datasets \"\"\"\n",
    "\n",
    "# English - French\n",
    "en_fr_ds = load_dataset('news_commentary', 'en-fr', split='train')\n",
    "\n",
    "# English - Persian (Farsi)\n",
    "en_fa_ds = load_dataset('persiannlp/parsinlu_translation_en_fa', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Preprocessing (English - French)\"\"\"\n",
    "\n",
    "en_fr_ds = en_fr_ds.map(lambda example: example['translation'])\\\n",
    "                   .remove_columns(['id', 'translation'])\\\n",
    "                   .rename_column('en', 'sentence')\\\n",
    "                   .rename_column('fr', 'translation')\\\n",
    "                   .select(range(300))\n",
    "\n",
    "# Tokenization\n",
    "en_fr_sentences = [mbert_tokenizer.tokenize(sentence)\n",
    "                   for sentence in en_fr_ds['sentence']]  # (List[List[str]])\n",
    "\n",
    "en_fr_translations = [mbert_tokenizer.tokenize(translation)\n",
    "                      for translation in en_fr_ds['translation']] # (List[List[str]])\n",
    "\n",
    "\n",
    "\"\"\" Preprocessing (English - Persian (Farsi)) \"\"\"\n",
    "\n",
    "# Removing the 'category' column\n",
    "en_fa_ds = en_fa_ds.remove_columns(['category'])\n",
    "\n",
    "# Removing list encapsulation\n",
    "en_fa_ds = en_fa_ds.map(lambda example: {'targets': example['targets'][0]}, num_proc=4)\n",
    "\n",
    "# Filtering out: \n",
    "# - rows with the '\\u200c' symbol,\n",
    "# - those where the length of either source or targets is less than a threshold\n",
    "# - Headlines (ending in 'Global Voices') --> because they are very short and the 'Global Voices' part is never translated\n",
    "length_threshold = 30\n",
    "filtered_en_fa_ds = en_fa_ds.filter(\n",
    "    lambda example: '\\u200c' not in example['targets']\n",
    "                    and len(example['source']) >= length_threshold\n",
    "                    and len(example['targets']) >= length_threshold\n",
    "                    and 'Global Voices' not in example['source'], \n",
    "    num_proc=4)\n",
    "\n",
    "en_fa_ds = filtered_en_fa_ds.rename_column('source', 'sentence')\\\n",
    "                            .rename_column('targets', 'translation')\\\n",
    "                            .select(range(300))\n",
    "\n",
    "# Tokenization\n",
    "en_fa_sentences = [mbert_tokenizer.tokenize(sentence)\n",
    "                   for sentence in en_fa_ds['sentence']]  # (List[List[str]])\n",
    "\n",
    "en_fa_translations = [mbert_tokenizer.tokenize(translation)\n",
    "                      for translation in en_fa_ds['translation']] # (List[List[str]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Running the Program (English - French)\"\"\"\n",
    "\n",
    "tune_en_fr_scores = [\n",
    "    BLANC_tune_translation(sentences, translations, mbert_checkpoint, mbert_model, mbert_tokenizer, device=DEVICE)\n",
    "    for sentences, translations in tqdm(zip(en_fr_sentences, en_fr_translations), total=len(en_fr_sentences))\n",
    "    ]\n",
    "\n",
    "# Saving the results\n",
    "en_fr_data = {}\n",
    "en_fr_data['BLANC_tune_en_fa_translation'] = tune_en_fr_scores\n",
    "add_results_to_json(en_fr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Running the Program (English - Persian)\"\"\"\n",
    "\n",
    "tune_en_fa_scores = [\n",
    "    BLANC_tune_translation(sentences, translations, mbert_checkpoint, mbert_model, mbert_tokenizer, device=DEVICE)\n",
    "    for sentences, translations in tqdm(zip(en_fa_sentences, en_fa_translations), total=len(en_fa_sentences))\n",
    "    ]\n",
    "\n",
    "# Saving the results\n",
    "en_fa_data = {}\n",
    "en_fa_data['BLANC_tune_en_fa_translation'] = tune_en_fa_scores\n",
    "add_results_to_json(en_fa_data)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
