{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# The models the authors used:\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "\n",
    "from blanc import BLANC_tune_summary, BLANC_tune_translation, add_results_to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "bert_checkpoint = \"bert-base-uncased\"\n",
    "bert_model = BertForMaskedLM.from_pretrained(bert_checkpoint).to(DEVICE)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_checkpoint, do_lower_case=True)\n",
    "\n",
    "mbert_checkpoint = \"bert-base-multilingual-uncased\"\n",
    "mbert_model = BertForMaskedLM.from_pretrained(mbert_checkpoint).to(DEVICE)\n",
    "mbert_tokenizer = BertTokenizer.from_pretrained(mbert_checkpoint, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLANC tune for **summaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Datasets \"\"\"\n",
    "\n",
    "DailyNews_ds = load_dataset(\"json\", data_files=\"DailyNews_300.json\", split=\"train\")\n",
    "DailyNews_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Preprocessing \"\"\"\n",
    "\n",
    "summaries = DailyNews_ds[\"summary\"]  # (List[str])\n",
    "texts = DailyNews_ds[\n",
    "    \"text\"\n",
    "]  # (List[str]) each string is a paragraph made of a few sentences\n",
    "\n",
    "# each text in texts is a list of sentences (each sentence is a string)\n",
    "texts = [sent_tokenize(text.strip()) for text in texts]  # List[List[str]]\n",
    "assert len(texts) == len(summaries) == 300\n",
    "\n",
    "tokenized_texts = [\n",
    "    [bert_tokenizer.tokenize(sentence) for sentence in text] for text in texts\n",
    "]  # List[List[List[str]]]\n",
    "tokenized_summaries = [\n",
    "    bert_tokenizer.tokenize(summary) for summary in summaries\n",
    "]  # [List[List[str]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Running the Program \"\"\"\n",
    "\n",
    "tune_summary_scores = [\n",
    "    BLANC_tune_summary(\n",
    "        text, summary, bert_checkpoint, bert_model, bert_tokenizer, device=DEVICE\n",
    "    )\n",
    "    for text, summary in tqdm(\n",
    "        zip(tokenized_texts, tokenized_summaries), total=len(tokenized_texts)\n",
    "    )\n",
    "]\n",
    "\n",
    "# Saving the results\n",
    "summary_data = {}\n",
    "summary_data[\"BLANC_tune_summary\"] = tune_summary_scores\n",
    "add_results_to_json(summary_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLANC tune for **translations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Datasets \"\"\"\n",
    "\n",
    "# English - French\n",
    "en_fr_ds = load_dataset(\"news_commentary\", \"en-fr\", split=\"train\")\n",
    "\n",
    "# English - Persian (Farsi)\n",
    "en_fa_ds = load_dataset(\"persiannlp/parsinlu_translation_en_fa\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Preprocessing (English - French)\"\"\"\n",
    "\n",
    "en_fr_ds = (\n",
    "    en_fr_ds.map(lambda example: example[\"translation\"])\n",
    "    .remove_columns([\"id\", \"translation\"])\n",
    "    .rename_column(\"en\", \"sentence\")\n",
    "    .rename_column(\"fr\", \"translation\")\n",
    "    .select(range(300))\n",
    ")\n",
    "\n",
    "# Tokenization\n",
    "en_fr_sentences = [\n",
    "    mbert_tokenizer.tokenize(sentence) for sentence in en_fr_ds[\"sentence\"]\n",
    "]  # (List[List[str]])\n",
    "\n",
    "en_fr_translations = [\n",
    "    mbert_tokenizer.tokenize(translation) for translation in en_fr_ds[\"translation\"]\n",
    "]  # (List[List[str]])\n",
    "\n",
    "\n",
    "\"\"\" Preprocessing (English - Persian (Farsi)) \"\"\"\n",
    "\n",
    "# Removing the 'category' column\n",
    "en_fa_ds = en_fa_ds.remove_columns([\"category\"])\n",
    "\n",
    "# Removing list encapsulation\n",
    "en_fa_ds = en_fa_ds.map(lambda example: {\"targets\": example[\"targets\"][0]}, num_proc=4)\n",
    "\n",
    "# Filtering out:\n",
    "# - rows with the '\\u200c' symbol,\n",
    "# - those where the length of either source or targets is less than a threshold\n",
    "# - Headlines (ending in 'Global Voices') --> because they are very short and the 'Global Voices' part is never translated\n",
    "length_threshold = 30\n",
    "filtered_en_fa_ds = en_fa_ds.filter(\n",
    "    lambda example: \"\\u200c\" not in example[\"targets\"]\n",
    "    and len(example[\"source\"]) >= length_threshold\n",
    "    and len(example[\"targets\"]) >= length_threshold\n",
    "    and \"Global Voices\" not in example[\"source\"],\n",
    "    num_proc=4,\n",
    ")\n",
    "\n",
    "en_fa_ds = (\n",
    "    filtered_en_fa_ds.rename_column(\"source\", \"sentence\")\n",
    "    .rename_column(\"targets\", \"translation\")\n",
    "    .select(range(300))\n",
    ")\n",
    "\n",
    "# Tokenization\n",
    "en_fa_sentences = [\n",
    "    mbert_tokenizer.tokenize(sentence) for sentence in en_fa_ds[\"sentence\"]\n",
    "]  # (List[List[str]])\n",
    "\n",
    "en_fa_translations = [\n",
    "    mbert_tokenizer.tokenize(translation) for translation in en_fa_ds[\"translation\"]\n",
    "]  # (List[List[str]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Running the Program (English - French)\"\"\"\n",
    "\n",
    "tune_en_fr_scores = [\n",
    "    BLANC_tune_translation(\n",
    "        sentences,\n",
    "        translations,\n",
    "        mbert_checkpoint,\n",
    "        mbert_model,\n",
    "        mbert_tokenizer,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    for sentences, translations in tqdm(\n",
    "        zip(en_fr_sentences, en_fr_translations), total=len(en_fr_sentences)\n",
    "    )\n",
    "]\n",
    "\n",
    "# Saving the results\n",
    "en_fr_data = {}\n",
    "en_fr_data[\"BLANC_tune_en_fa_translation\"] = tune_en_fr_scores\n",
    "add_results_to_json(en_fr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Running the Program (English - Persian)\"\"\"\n",
    "\n",
    "tune_en_fa_scores = [\n",
    "    BLANC_tune_translation(\n",
    "        sentences,\n",
    "        translations,\n",
    "        mbert_checkpoint,\n",
    "        mbert_model,\n",
    "        mbert_tokenizer,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    for sentences, translations in tqdm(\n",
    "        zip(en_fa_sentences, en_fa_translations), total=len(en_fa_sentences)\n",
    "    )\n",
    "]\n",
    "\n",
    "# Saving the results\n",
    "en_fa_data = {}\n",
    "en_fa_data[\"BLANC_tune_en_fa_translation\"] = tune_en_fa_scores\n",
    "add_results_to_json(en_fa_data)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
